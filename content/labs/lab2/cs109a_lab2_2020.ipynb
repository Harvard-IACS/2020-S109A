{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS-109A Introduction to Data Science\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Linear Regression and k-NN\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2019**<br/>\n",
    "**Authors:** Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas, Chris Tanner, Arpit Panda\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> Building a model with statsmodels and sklearn </li>\n",
    "<li> Example: simple linear regression with automobile data </li>\n",
    "<li> $k$-nearest neighbors</li>\n",
    "<li> Polynomial Regression, and the Cab Data</li>\n",
    "<li> Multiple regression and exploring the Football data </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "After this lab, you should be able to\n",
    "- Feel comfortable with simple linear regression\n",
    "- Feel comfortable with $k$ nearest neighbors\n",
    "- Explain the difference between train/validation/test data and WHY we have each.\n",
    "- Implement arbitrary multiple regression models in both SK-learn and Statsmodels.\n",
    "- Interpret the coefficent estimates produced by each model, including transformed and dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import the necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.api import OLS\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "## Part 1 - Simple Linear Regression with `statsmodels` and `sklearn`\n",
    "\n",
    "Let's learn two `python` packages to do simple linear regression for us:\n",
    "* [statsmodels](http://www.statsmodels.org/stable/regression.html) and \n",
    "* [scikit-learn (sklearn)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
    "\n",
    "Our goal  is to show how to implement simple linear regression with these packages.  \n",
    "\n",
    "For the purposes of this lab, `statsmodels` and `sklearn` do the same thing.  More generally though, `statsmodels` tends to be easier for inference \\[finding the values of the slope and intercept and dicussing uncertainty in those values\\], whereas `sklearn` has machine-learning algorithms and is better for prediction \\[guessing y values for a given x value\\]. (Note that both packages make the same guesses, it's just a question of which activity they provide more support for.\n",
    "\n",
    "**Note:** `statsmodels` and `sklearn` are different packages!  Unless we specify otherwise, you can use either one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with a toy dataset\n",
    "We first examine a toy problem, focusing our efforts on fitting a linear model to a small dataset with three observations.  Each observation consists of one predictor $x_i$ and one response $y_i$ for $i = 1, 2, 3$,\n",
    "\n",
    "\\begin{align*}\n",
    "(x , y) = \\{(x_1, y_1), (x_2, y_2), (x_3, y_3)\\}.\n",
    "\\end{align*}\n",
    "\n",
    "To be very concrete, let's set the values of the predictors and responses.\n",
    "\n",
    "\\begin{equation*}\n",
    "(x , y) = \\{(1, 2), (2, 2), (3, 4)\\}\n",
    "\\end{equation*}\n",
    "\n",
    "There is no line of the form $\\beta_0 + \\beta_1 x = y$ that passes through all three observations, since the data are not collinear. Thus our aim is to find the line that best fits these observations in the *least-squares sense*, as discussed in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Q1.1</b></div>\n",
    "\n",
    "* Make two numpy arrays out of this data, x_train and y_train\n",
    "* Check the dimentions of these arrays\n",
    "* Be sure to reshape input array to 2-D because this becomes important later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for `statsmodels`.  `Statsmodels` does not by default include the column of ones in the $X$ matrix, so we include it manually with `sm.add_constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the X matrix by appending a column of ones to x_train\n",
    "X = sm.add_constant(x_ train)\n",
    "\n",
    "\n",
    "# build the OLS model (ordinary least squares) from the training data\n",
    "toyregr_sm = sm.OLS(y_train, X)\n",
    "\n",
    "# do the fit and save regression info (parameters, etc) in results_sm\n",
    "results_sm = toyregr_sm.fit()\n",
    "\n",
    "# pull the beta parameters out from results_sm\n",
    "beta0_sm = results_sm.params[0]\n",
    "beta1_sm = results_sm.params[1]\n",
    "\n",
    "print(f'The regression coef from statsmodels are: beta_0 = {beta0_sm:8.6f} and beta_1 = {beta1_sm:8.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the beta parameters, `results_sm` contains a ton of other potentially useful information."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(results_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's turn our attention to the `sklearn` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the least squares model\n",
    "toyregr = linear_model.LinearRegression()\n",
    "# save regression info (parameters, etc) in results_skl\n",
    "results = toyregr.fit(x_train, y_train)\n",
    "\n",
    "# pull the beta parameters out from results_skl\n",
    "beta0_skl = toyregr.intercept_\n",
    "beta1_skl = toyregr.coef_[0]\n",
    "\n",
    "print(\"The regression coefficients from the sklearn package are: beta_0 = {0:8.6f} and beta_1 = {1:8.6f}\".format(beta0_skl, beta1_skl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Q1.2</b></div>\n",
    "Do the values of beta_0 and beta_1 seem reasonable?\n",
    "Plot the training data using a scatter plot.\n",
    "Plot the best fit line with beta0 and beta1 together with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should feel pretty good about ourselves now, and we're ready to move on to a real problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use `scikit-learn`\n",
    "Before diving right in to a \"real\" problem, we really ought to discuss more of the details of `sklearn`.  We do this now.  Along the way, we'll import the real-world dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Scikit-learn` is the main `python` machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as `train_test_split`. It can be used in `python` by the incantation `import sklearn`.\n",
    "\n",
    "In scikit-learn, an **estimator** is a Python object that implements the methods fit(X, y) and predict(T)\n",
    "\n",
    "Let's see the structure of `scikit-learn` needed to make these fits. `.fit` always takes two arguments:\n",
    "```python\n",
    "  estimator.fit(Xtrain, ytrain)\n",
    "```\n",
    "We will consider two estimators in this lab: `LinearRegression` and `KNeighborsRegressor`.\n",
    "\n",
    "Critically, `Xtrain` must be in the form of an *array of arrays* (or a 2x2 array) with the inner arrays each corresponding to one sample, and whose elements correspond to the feature values for that sample (visuals coming in a moment).\n",
    "\n",
    "`ytrain` on the other hand is a simple array of responses.  These are continuous for regression problems.\n",
    "\n",
    "![](images/sklearn2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice with `sklearn`\n",
    "We begin by loading up the `mtcars` dataset and cleaning it up a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#load mtcars\n",
    "dfcars = pd.read_csv(\"../data/mtcars.csv\")\n",
    "dfcars = dfcars.rename(columns={\"Unnamed: 0\":\"car name\"})\n",
    "dfcars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split the dataset into a training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training set and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#set random_state to get the same split every time\n",
    "traindf, testdf = train_test_split(dfcars, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing set is around 20% of the total data; training set is around 80%\n",
    "print(\"Shape of full dataset is: {0}\".format(dfcars.shape))\n",
    "print(\"Shape of training dataset is: {0}\".format(traindf.shape))\n",
    "print(\"Shape of test dataset is: {0}\".format(testdf.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have training and test data.  We still need to select a predictor and a response from this dataset.  Keep in mind that we need to choose the predictor and response from both the training and test set.  You will do this in the exercises below.  However, we provide some starter code for you to get things going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the response variable that we're interested in\n",
    "y_train = traindf.mpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's reshape y_train to be an array of arrays using the reshape method. We want the first dimension of y_train to be size  2525  and the second dimension to be size  11 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_reshape = y_train.values.reshape(-1,1)\n",
    "y_train_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear regression with automobile data\n",
    "We will now use `sklearn` to predict automobile mileage per gallon (mpg) and evaluate these predictions. We already loaded the data and split them into a training set and a test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Q1.3</b></div>\n",
    "* Pick one variable to use as a predictor for simple linear regression.  Create a markdown cell below and discuss your reasons.  \n",
    "* Justify your choice with some visualizations.  \n",
    "* Is there a second variable you'd like to use? For example, we're not doing multiple linear regression here, but if we were, is there another variable you'd like to include if we were using two predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Q1.4</b></div>\n",
    "\n",
    "* Use `sklearn` to fit the training data using simple linear regression.\n",
    "* Use the model to make mpg predictions on the test set.  \n",
    "\n",
    "**Hints:**\n",
    "* Use the following to perform the analysis:\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>Q1.5</b></div>\n",
    "* Plot the data and the prediction.  \n",
    "* Print out the mean squared error for the training set and the test set and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - $k$-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're familiar with `sklearn`, you're ready to do a KNN regression.  \n",
    "\n",
    "Sklearn's regressor is called `sklearn.neighbors.KNeighborsRegressor`. Its main parameter is the `number of nearest neighbors`. There are other parameters such as the distance metric (default for 2 order is the Euclidean distance). For a list of all the parameters see the [Sklearn kNN Regressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html).\n",
    "\n",
    "Let's use $5$ nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set number of neighbors\n",
    "k = 5\n",
    "knnreg = KNeighborsRegressor(n_neighbors=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>2.1</b></div>\n",
    "Fit the regressor and calculate the model's prediction on the test set and print the $R^{2}$ score  (try not to look at the cell below before you try it yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not so good? Lets vary the number of neighbors and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make our lives easy by storing the different regressors in a dictionary\n",
    "regdict = {}\n",
    "\n",
    "# Make our lives easier by entering the k values from a list\n",
    "k_list = [1, 2, 4, 15]\n",
    "\n",
    "# Do a bunch of KNN regressions\n",
    "for k in k_list:\n",
    "    knnreg = KNeighborsRegressor(n_neighbors=k)\n",
    "    knnreg.fit(X_train, y_train)\n",
    "    # Store the regressors in a dictionary\n",
    "    regdict[k] = knnreg \n",
    "\n",
    "# Print the dictionary to see what we have\n",
    "regdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot all the k values in same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "\n",
    "ax.plot(dfcars.wt, dfcars.mpg, 'o', label=\"data\")\n",
    "\n",
    "xgrid = np.linspace(np.min(dfcars.wt), np.max(dfcars.wt), 100)\n",
    "\n",
    "# let's unpack the dictionary to its elements (items) which is the k and Regressor\n",
    "for k, regressor in regdict.items():\n",
    "    predictions = regressor.predict(xgrid.reshape(-1,1)) \n",
    "    ax.plot(xgrid, predictions, label=\"{}-NN\".format(k))\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>2.2</b></div>\n",
    "\n",
    "Explain what you see in the graph. **Hint** Notice how the $1$-NN goes through every point on the training set but utterly fails elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your explanation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the scores on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ks = range(1, 15) # Grid of k's\n",
    "scores_train = [] # R2 scores\n",
    "for k in ks:\n",
    "    # Create KNN model\n",
    "    knnreg = KNeighborsRegressor(n_neighbors=k) \n",
    "    \n",
    "    # Fit the model to training data\n",
    "    knnreg.fit(X_train, y_train) \n",
    "    \n",
    "    # Calculate R^2 score\n",
    "    score_train = knnreg.score(X_train, y_train) \n",
    "    scores_train.append(score_train)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(12,8))\n",
    "ax.plot(ks, scores_train,'o-')\n",
    "ax.set_xlabel(r'$k$')\n",
    "ax.set_ylabel(r'$R^{2}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>2.3</div>\n",
    "\n",
    "* Why do we get a perfect $R^2$ at k=1 for the training set?\n",
    "* Make the same plot as above on the *test* set.\n",
    "* What is the best $k$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Polynomial Regression, and Exploring the Cab Data\n",
    "\n",
    "Polynomial regression uses a **linear model** to estimate a **non-linear function** (i.e., a function with polynomial terms). For example:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_i + \\beta_1x_i^{2}$\n",
    "\n",
    "It is a linear model because we are still solving a linear equation (the _linear_ aspect refers to the beta coefficients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in the data, break into train and test\n",
    "cab_df = pd.read_csv(\"../data/dataset_1.txt\")\n",
    "train_data, test_data = train_test_split(cab_df, test_size=.2, random_state=42)\n",
    "cab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cab_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do some data cleaning\n",
    "X_train = train_data['TimeMin'].values.reshape(-1,1)/60 # transforms it to being hour-based\n",
    "y_train = train_data['PickupCount'].values\n",
    "\n",
    "X_test = test_data['TimeMin'].values.reshape(-1,1)/60 # hour-based\n",
    "y_test = test_data['PickupCount'].values\n",
    "\n",
    "def plot_cabs(cur_model, poly_transformer=None):\n",
    "    \n",
    "    # build the x values for the prediction line\n",
    "    x_vals = np.arange(0,24,.1).reshape(-1,1)\n",
    "    \n",
    "    # optionally use the passed-in transformer\n",
    "    if poly_transformer != None:\n",
    "        dm = poly_transformer.fit_transform(x_vals)\n",
    "    else:\n",
    "        dm = x_vals\n",
    "        \n",
    "    # make the prediction at each x value\n",
    "    prediction = cur_model.predict(dm)\n",
    "    \n",
    "    # plot the prediction line, and the test data\n",
    "    plt.plot(x_vals,prediction, color='k', label=\"Prediction\")\n",
    "    plt.scatter(X_test, y_test, label=\"Test Data\")\n",
    "\n",
    "    # label your plots\n",
    "    plt.ylabel(\"Number of Taxi Pickups\")\n",
    "    plt.xlabel(\"Time of Day (Hours Past Midnight)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "fitted_cab_model0 = LinearRegression().fit(X_train, y_train)\n",
    "plot_cabs(fitted_cab_model0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fitted_cab_model0.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there's still a lot of variation in cab pickups that's not being captured by a linear fit. Further, the linear fit is predicting massively more pickups at 11:59pm than at 12:00am. This is a bad property, and it's the conseqeuence of having a straight line with a non-zero slope. However, we can add columns to our data for $TimeMin^2$ and $TimeMin^3$ and so on, allowing a curvy polynomial line to hopefully fit the data better.\n",
    "\n",
    "We'll be using ``sklearn``'s `PolynomialFeatures()` function to take some of the tedium out of building the expanded input data. In fact, if all we want is a formula like $y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ...$, it will directly return a new copy of the data in this format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transformer_3 = PolynomialFeatures(3, include_bias=False)\n",
    "expanded_train = transformer_3.fit_transform(X_train) # TRANSFORMS it to polynomial features\n",
    "pd.DataFrame(expanded_train).describe() # notice that the columns now contain x, x^2, x^3 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on `PolynomialFeatures`:\n",
    "\n",
    "- The interface is a bit strange. `PolynomialFeatures` is a _'transformer'_ in sklearn. We'll be using several transformers that learn a transformation on the training data, and then we will apply those transformations on future data. With PolynomialFeatures, the `.fit()` is pretty trivial, and we often fit and transform in one command, as seen above with ``.fit_transform()`.\n",
    "- You rarely want to `include_bias` (a column of all 1's), since _**sklearn**_ will add it automatically. Remember, when using _**statsmodels,**_ you can just `.add_constant()` right before you fit the data.\n",
    "- If you want polynomial features for a several different variables (i.e., multinomial regression), you should call `.fit_transform()` separately on each column and append all the results to a copy of the data (unless you also want interaction terms between the newly-created features). See `np.concatenate()` for joining arrays."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "fitted_cab_model3 = LinearRegression().fit(expanded_train, y_train)\n",
    "print(\"fitting expanded_train:\", expanded_train)\n",
    "plot_cabs(fitted_cab_model3, transformer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>3.1</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. Calculate the polynomial model's $R^2$ performance on the test set. \n",
    "2. Does the polynomial model improve on the purely linear model?\n",
    "3. Make a residual plot for the polynomial model. What does this plot tell us about the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# NOTE 1: unlike statsmodels' r2_score() function, sklearn has a .score() function\n",
    "# NOTE 2: fit_transform() is a nifty function that transforms the data, then fits it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANSWER 2: does it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ANSWER 3 (class discussion about the residuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other features\n",
    "Polynomial features are not the only constucted features that help fit the data. Because these data have a 24 hour cycle, we may want to build features that follow such a cycle. For example, $sin(24\\frac{x}{2\\pi})$, $sin(12\\frac{x}{2\\pi})$, $sin(8\\frac{x}{2\\pi})$. Other feature transformations are appropriate to other types of data. For instance certain feature transformations have been developed for geographical data.\n",
    "\n",
    "### Scaling Features\n",
    "When using polynomials, we are explicitly trying to use the higher-order values for a given feature. However, sometimes these polynomial features can take on values that are drastically large, making it difficult for the system to learn an appropriate bias weight due to its large values and potentially large variance. To counter this, sometimes one may be interested in scaling the values for a given feature.\n",
    "\n",
    "For our ongoing taxi-pickup example, using polynomial features improved our model. If we wished to scale the features, we could use `sklearn`'s StandardScaler() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SCALES THE EXPANDED/POLY TRANSFORMED DATA\n",
    "# we don't need to convert to a pandas dataframe, but it can be useful for scaling select columns\n",
    "train_copy = pd.DataFrame(expanded_train.copy())\n",
    "test_copy = pd.DataFrame(expanded_test.copy())\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler = StandardScaler().fit(train_copy)\n",
    "\n",
    "# Scale both the test and training data. \n",
    "train_scaled = scaler.transform(expanded_train)\n",
    "test_scaled = scaler.transform(expanded_test)\n",
    "\n",
    "# we could optionally run a new regression model on this scaled data\n",
    "fitted_scaled_cab = LinearRegression().fit(train_scaled, y_train)\n",
    "fitted_scaled_cab.score(test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">\n",
    "\n",
    "## Part 4: Multiple regression and exploring the Football (aka soccer) data\n",
    "Let's move on to a different dataset! The data imported below were scraped by [Shubham Maurya](https://www.kaggle.com/mauryashubham/linear-regression-to-predict-market-value/data) and record various facts about players in the English Premier League. Our goal will be to fit models that predict the players' market value (how much would a team pay for their services), as estimated by https://www.transfermarkt.us.\n",
    "\n",
    "`name`: Name of the player  \n",
    "`club`: Club of the player  \n",
    "`age` : Age of the player  \n",
    "`position` : The usual position on the pitch  \n",
    "`position_cat` :  1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers  \n",
    "`market_value` : As on transfermrkt.com on July 20th, 2017  \n",
    "`page_views` : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017  \n",
    "`fpl_value` : Value in Fantasy Premier League as on July 20th, 2017  \n",
    "`fpl_sel` : % of FPL players who have selected that player in their team  \n",
    "`fpl_points` : FPL points accumulated over the previous season  \n",
    "`region`: 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World  \n",
    "`nationality`: Player's nationality  \n",
    "`new_foreign`: Whether a new signing from a different league, for 2017/18 (till 20th July)  \n",
    "`age_cat`: a categorical version of the Age feature  \n",
    "`club_id`: a numerical version of the Club feature  \n",
    "`big_club`: Whether one of the Top 6 clubs  \n",
    "`new_signing`: Whether a new signing for 2017/18 (till 20th July)  \n",
    "\n",
    "As always, we first import, verify, split, and explore the data.\n",
    "\n",
    "## Import and verification and grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "league_df = pd.read_csv(\"../data/league_data.txt\")\n",
    "print(league_df.dtypes)\n",
    "\n",
    "# QUESTION: what would you guess is the mean age? mean salary?\n",
    "#league_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "league_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#league_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Stratified) train/test split\n",
    "We want to make sure that the training and test data have appropriate representation of each region; it would be bad for the training data to entirely miss a region. This is especially important because some regions are rather rare.\n",
    "\n",
    "<div class=\"exercise\"><b>4.1</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. Use the `train_test_split()` function, while (a) ensuring the test size is 20% of the data, and; (2) using 'stratify' argument to split the data (look up documentation online), keeping equal representation of each region. This doesn't work by default, correct? What is the issue?\n",
    "2. Deal with the issue you encountered above. Hint: you may find numpy's `.isnan()` and panda's `.dropna()` functions useful!\n",
    "3. How did you deal with the error generated by `train_test_split`? How did you justify your action? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we won't be peeking at the test set, let's explore and look for patterns! We'll introduce a number of useful pandas and numpy functions along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "Pandas' `.groupby()` function is a wonderful tool for data analysis. It allows us to analyze each of several subgroups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>4.2</b></div>\n",
    "What is the average market value, median page views, and maximum fpl for each player position?\n",
    "\n",
    "Try repeating this by additionally inspecting if this changes by whether the player plays at a big club or not? So, include \"big club\" in your groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:3px\">\n",
    "\n",
    "## Linear regression on the football data\n",
    "This section of the lab focuses on fitting a model to the football (soccer) data and interpreting the model results. The model we'll use is\n",
    "\n",
    "$$\\text{market_value} \\approx \\beta_0 + \\beta_1\\text{fpl_points} + \\beta_2\\text{age} + \\beta_3\\text{age}^2 + \\beta_4log_2\\left(\\text{page_views}\\right) + \\beta_5\\text{new_signing} +\\beta_6\\text{big_club} + \\beta_7\\text{position_cat}$$\n",
    "\n",
    "We're including a 2nd degree polynomial in age because we expect pay to increase as a player gains experience, but then decrease as they continue aging. We're taking the log of page views because they have such a large, skewed range and the transformed variable will have fewer outliers that could bias the line. We choose the base of the log to be 2 just to make interpretation cleaner.\n",
    "\n",
    "<div class=\"exercise\"><b>4.3</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. Build the data and fit this model to it. How good is the overall model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q1: we'll do most of it for you ...\n",
    "y_train = train_data['market_value']\n",
    "y_test = test_data['market_value']\n",
    "def build_football_data(df):\n",
    "    x_matrix = df[['fpl_points','age','new_signing','big_club','position_cat']].copy()\n",
    "    x_matrix['log_views'] = np.log2(df['page_views'])\n",
    "    \n",
    "    # WRITE CODE FOR CREATING THE AGE SQUARED COLUMN\n",
    "    ####\n",
    "    \n",
    "    # OPTIONALLY WRITE CODE to adjust the ordering of the columns, just so that it corresponds with the equation above\n",
    "    ####\n",
    "    \n",
    "    # add a constant\n",
    "    x_matrix = sm.add_constant(x_matrix)\n",
    "    \n",
    "    return x_matrix\n",
    "\n",
    "# use build_football_data() to transform both the train_data and test_data\n",
    "train_transformed = build_football_data(train_data)\n",
    "test_transformed = build_football_data(test_data)\n",
    "\n",
    "fitted_model_1 = OLS(endog= y_train, exog=train_transformed, hasconst=True).fit()\n",
    "fitted_model_1.summary()\n",
    "\n",
    "# WRITE CODE TO RUN r2_score(), then answer the above question about the overall goodness of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>4.4</b></div>\n",
    " Interpret the regression model. What is the meaning of the coefficient for:\n",
    "    - age and age$^2$\n",
    "    - $log_2($page_views$)$\n",
    "    - big_club\n",
    "    \n",
    " Plot the effect of age value on market value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "\n",
    "# let's set our x-axis (corresponding to age) to be a wide range from -100 to 100, \n",
    "# just to see a grand picture of the function\n",
    "x_vals = np.linspace(-100,100,1000)\n",
    "y_vals = agecoef*x_vals +age2coef*x_vals**2\n",
    "\n",
    "# WRITE CODE TO PLOT x_vals vs y_vals\n",
    "plt.plot(x_vals, y_vals)\n",
    "plt.title(\"Effect of Age\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Contribution to Predicted Market Value\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>4.5</b></div>\n",
    "What happens if we only use age (not age^2) in our model (what's the r2?); make the same plot of age vs market value\n",
    " What happens if we only use age^2 (not age) in our model (what's the r2?); make the same plot of age^2 vs market value\n",
    " PLOT page views vs market value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>4.6</b></div>\n",
    "What should a player do in order to improve their market value? How many page views should a player go get to increase their market value by 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"exercise\"><b>4.7</b></div>\n",
    "How could you model the data to answer the question: 'Are aging curves different depending on position (do defensive players peak at a different age, for example)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style='height:3px'>\n",
    "\n",
    "### Turning Categorical Variables into multiple binary variables\n",
    "Of course, we have an error in how we've included player position. Even though the variable is numeric (1,2,3,4) and the model runs without issue, the value we're getting back is garbage. The interpretation, such as it is, is that there is an equal effect of moving from position category 1 to 2, from 2 to 3, and from 3 to 4, and that this effect is probably between -0.5 to -1 (depending on your run).\n",
    "\n",
    "In reality, we don't expect moving from one position category to another to be equivalent, nor for a move from category 1 to category 3 to be twice as important as a move from category 1 to category 2. We need to introduce better features to model this variable.\n",
    "\n",
    "We'll use `pd.get_dummies` to do the work for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>fpl_points</th>\n",
       "      <th>age</th>\n",
       "      <th>new_signing</th>\n",
       "      <th>big_club</th>\n",
       "      <th>log_views</th>\n",
       "      <th>position_cat_2</th>\n",
       "      <th>position_cat_3</th>\n",
       "      <th>position_cat_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.497852</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.0</td>\n",
       "      <td>92</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.738092</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.722808</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.277287</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     const  fpl_points  age  new_signing  big_club  log_views  position_cat_2  position_cat_3  position_cat_4\n",
       "107    1.0          15   22            0         1   9.497852               0               1               0\n",
       "118    1.0          92   31            0         0   8.738092               0               0               0\n",
       "444    1.0          89   27            1         0   9.722808               0               0               0\n",
       "182    1.0           0   25            0         0   9.000000               0               0               0\n",
       "233    1.0           5   17            0         1  10.277287               0               0               0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_design_recoded = pd.get_dummies(train_transformed, columns=['position_cat'], drop_first=True)\n",
    "test_design_recoded = pd.get_dummies(test_transformed, columns=['position_cat'], drop_first=True)\n",
    "\n",
    "train_design_recoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've removed the original `position_cat` column and created three new ones.\n",
    "\n",
    "#### Why only three new columns?\n",
    "Why does pandas give us the option to drop the first category? \n",
    "\n",
    "<div class=\"exercise\"><b>4.8</b></div>\n",
    "\n",
    "**Questions**:\n",
    "1. If we're fitting a model without a constant, should we have three dummy columns or four dummy columns?\n",
    "2. Fit a model on the new, recoded data, then interpret the coefficient of `position_cat_2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS EXERCISE:\n",
    "We have provided a spreadsheet of Boston housing prices (data/boston_housing.csv). The 14 columns are as follows:\n",
    "1. CRIM: per capita crime rate by town\n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "3. INDUS: proportion of non-retail business acres per town\n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "5. NOX: nitric oxides concentration (parts per 10 million) 1https://archive.ics.uci.edu/ml/datasets/Housing 123 20.2. Load the Dataset 124\n",
    "6. RM: average number of rooms per dwelling\n",
    "7. AGE: proportion of owner-occupied units built prior to 1940\n",
    "8. DIS: weighted distances to ﬁve Boston employment centers\n",
    "9. RAD: index of accessibility to radial highways\n",
    "10. TAX: full-value property-tax rate per \\$10,000\n",
    "11. PTRATIO: pupil-teacher ratio by town\n",
    "12. B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town\n",
    "13. LSTAT: % lower status of the population\n",
    "14. MEDV: Median value of owner-occupied homes in $1000s We can see that the input attributes have a mixture of units\n",
    "\n",
    "There are 450 observations.\n",
    "<div class=\"exercise\"><b>Bonus Exercise</b></div>\n",
    "\n",
    "Using the above file, try your best to predict **housing prices. (the 14th column)** We have provided a test set `data/boston_housing_test.csv` but refrain from looking at the file or evaluating on it until you have finalized and trained a model.\n",
    "1. Load in the data. It is tab-delimited. Quickly look at a summary of the data to familiarize yourself with it and ensure nothing is too egregious.\n",
    "2. Use a previously-discussed function to automatically partition the data into a training and validation (aka development) set. It is up to you to choose how large these two portions should be.\n",
    "3. Train a basic model on just a subset of the features. What is the performance on the validation set?\n",
    "4. Train a basic model on all of the features. What is the performance on the validation set?\n",
    "5. Toy with the model until you feel your results are reasonably good.\n",
    "6. Perform cross-validation with said model, and measure the average performance. Are the results what you expected? Were the average results better or worse than that from your original 1 validation set?\n",
    "7. Experiment with other models, and for each, perform 10-fold cross-validation. Which model yields the best average performance? Select this as your final model.\n",
    "8. Use this model to evaulate your performance on the testing set. What is your performance (MSE)? Is this what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python p3_env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
