{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2018-CS109A/blob/master/content/styles/iacs.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 8: Trees and Forests\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2020**<br>\n",
    "**Instructors:** Kevin Rader<br>\n",
    "**Authors:** Rahul Dave, David Sondak, Pavlos Protopapas, Chris Tanner, Eleni Kaxiras, Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Decision Trees </li> \n",
    "<li> Bootstrap-Aggregated (Bagged) Trees </li> \n",
    "<li> Random Forests </li> \n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 8. By the end of this lecture, you should be able to:\n",
    "\n",
    "- Be able to fit and visualize (through predictions) decision trees, bootstrap-aggregated (bagged) trees, and random forests. \n",
    "- Have a general sense of the difference between the 3 tree based methods.\n",
    "- Tune the hyperparameters of a random forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import tree\n",
    "import sklearn.metrics as met\n",
    "\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Reading and Exploring the data \n",
    "\n",
    "We will be using the same data as lectures 3-5 (both the train and test splits): modeling `votergap` from the 2016 election (Trump-Clinton) from many predictors in the `county_election` dataset.\n",
    "\n",
    "We start by reading in the datasets for you and visualizing the main predictors for now: `minority`:\n",
    "\n",
    "**Important note: use the training dataset for all exploratory analysis and model fitting.  Only use the test dataset to evaluate and compare models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elect_train = pd.read_csv(\"../data/county_election_train.csv\")\n",
    "elect_test = pd.read_csv(\"../data/county_election_test.csv\")\n",
    "elect_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = elect_train['votergap']\n",
    "y_test = elect_test['votergap']\n",
    "\n",
    "plt.hist(y_train)\n",
    "elect_train.hist(column=['minority', 'density','obesity','female']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q0.1** How would you describe these variables?  What issues does this create?  How can these issues be fixed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elect_train.shape)\n",
    "print(elect_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(elect_train['minority'],y_train, '.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(elect_train['minority']),y_train, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q0.2** Which of the two versions of 'minority' would be a better choice to use as a predictor for inference?  For prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Decision Trees\n",
    "\n",
    "We could use a simple Decision Tree regressor to predict votergap. That's not the aim of this lab, so we'll run a few of these models without any cross-validation or 'regularization' just to illustrate what is going on.\n",
    "\n",
    "This is what you ought to keep in mind about decision trees.\n",
    "\n",
    "from the docs:\n",
    "```\n",
    "max_depth : int or None, optional (default=None)\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "min_samples_split : int, float, optional (default=2)\n",
    "```\n",
    "\n",
    "- The deeper the tree, the more prone you are to overfitting.\n",
    "- The smaller `min_samples_split`, the more the overfitting. One may use `min_samples_leaf` instead. More samples per leaf, the higher the bias (aka, simpler, underfit model).\n",
    "\n",
    "Below we fit 2 decision treees that limit the `max_depth`: a single split, and one with 2 splits (resulting in 4 leaves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "elect_train['logminority'] = np.log(elect_train['minority'])\n",
    "elect_test['logminority'] = np.log(elect_test['minority'])\n",
    "\n",
    "dummy_x = np.arange(np.min(elect_train['logminority']),np.max(elect_train['logminority']),0.01)\n",
    "\n",
    "plt.plot(elect_train['logminority'],y_train,'.')\n",
    "\n",
    "for i in [1,2]:\n",
    "    dtree = DecisionTreeRegressor(max_depth=i)\n",
    "    dtree.fit(elect_train[['logminority']],y_train)\n",
    "    plt.plot(dummy_x , dtree.predict(dummy_x.reshape(-1,1)), label=(\"max depth =\"+str(i)), alpha=0.8, lw=4)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the actual decision tree can be *printed out* using [sklearn.tree.plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree.plot_tree(dtree, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1** Interpret the printed out tree above: how does it match the scatterplot visualization of the regression tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2** Play around with the various arguments to define the complexity of the decision tree: `max_depth`,`min_samples_split`, and `min_samples_leaf` (do 1 at a time for now, you can use multiple of these arguments).  Roughly, at what point do these start to overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3** Perform 5-fold cross-validation to determine what the best `max_depth` would be for a single regression tree using the entire `X_train` feature set defined below.  Visualize the results with mean +/- 2 sd's across the validation sets.  Interpret the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "elect_train['logdensity'] = np.log(elect_train['density'])\n",
    "elect_train['loghispanic'] = np.log(elect_train['hispanic'])\n",
    "\n",
    "elect_test['logdensity'] = np.log(elect_test['density'])\n",
    "elect_test['loghispanic'] = np.log(elect_test['hispanic'])\n",
    "\n",
    "X_train = elect_train[['logminority', 'logdensity','loghispanic','obesity','female','income','bachelor','inactivity']]\n",
    "X_test = elect_test[['logminority', 'logdensity','loghispanic','obesity','female','income','bachelor','inactivity']]\n",
    "\n",
    "\n",
    "depths = list(range(1, 21))\n",
    "train_scores = []\n",
    "cvmeans = []\n",
    "cvstds = []\n",
    "cv_scores = []\n",
    "for depth in depths:\n",
    "    dtree = DecisionTreeRegressor(max_depth=depth)\n",
    "    ######\n",
    "    # your code here: \n",
    "    # Perform 5-fold cross validation and store results\n",
    "    ######\n",
    "    \n",
    "\n",
    "#cvmeans = np.array(cvmeans)\n",
    "#cvstds = np.array(cvstds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here: \n",
    "# plot means and shade the 2 SD interval\n",
    "######\n",
    "# hint: use 'plt.fill_between' (with 'alpha' around 0.3) to plot the +/- 2sd's of CV scores behind the means.\n",
    "\n",
    "#plt.legend()\n",
    "#plt.ylabel(\"Accuracy\")\n",
    "#plt.xlabel(\"Max Depth\")\n",
    "#plt.xticks(depths);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the plot above, it's clear that the training $R^2$ increases towards one as `max_depth` increases, while the test set's $R^2$ maxes out around 50% around a `max_depth` of 7. Any of those values would be reasonable to choose for a best predictive model.  To prevent overfitting and for parsimony, I would choose a `max_depth` of 7.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok with this discussion in mind, lets improve this model by Bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bootstrap-Aggregating (called Bagging)\n",
    "\n",
    "Whats the basic idea?\n",
    "\n",
    "- A Single Decision tree is likely to overfit and is very 'jumpy' (discrete step-function).\n",
    "- So lets introduce replication through Bootstrap sampling.\n",
    "- **Bagging** uses bootstrap resampling to create different training datasets. This way each training will give us a different tree.\n",
    "- Added bonus: the left off points can be used to as a natural \"validation\" set, so no need to \n",
    "- Since we have many trees that we will **average over for prediction**, we can choose a large `max_depth` and we are ok as we will rely on the law of large numbers to shrink this large variance, low bias approach for each individual tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "ntrees = 200\n",
    "estimators = []\n",
    "R2s = []\n",
    "yhats_test = np.zeros((X_test.shape[0], ntrees))\n",
    "\n",
    "plt.plot(elect_train['logminority'],y_train,'.')\n",
    "\n",
    "for i in range(ntrees):\n",
    "    simpletree = DecisionTreeRegressor(max_depth=5)\n",
    "    boot_x, boot_y = resample(X_train[['logminority']], y_train)\n",
    "    estimators = np.append(estimators,simpletree.fit(boot_x, boot_y))\n",
    "    R2s = np.append(R2s,simpletree.score(X_test[['logminority']], y_test))\n",
    "    yhats_test[:,i] = simpletree.predict(X_test[['logminority']])\n",
    "    plt.plot(dummy_x, simpletree.predict(dummy_x.reshape(-1,1)), 'red', alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1** Interpret the plot above: how does bagging improve a single decision tree?  How could the resulting models be used to perform prediction?  Should the base trees be underfit, overfit, or just right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** Do lots of work here:\n",
    "1. Edit the code below (which is just copied from above) to refit many bagged trees on the entire `X_train` feature set (without the plot...lots of predictors now so difficult to plot). \n",
    "2. Summarize how each of the separate trees performed (both numerically and visually) using $R^2$ as the metric.  How do they perform on average?\n",
    "3. Combine the trees into one prediction and evaluate it using $R^2$.\n",
    "4. Briefly discuss the results.  How will the results above change if 'max_depth' is increased?  What if it is decreased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4f623538be50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mestimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mR2s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0my_hats_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "######\n",
    "# edit the code below\n",
    "######\n",
    "\n",
    "\n",
    "ntrees = 200\n",
    "estimators = []\n",
    "R2s = []\n",
    "y_hats_test = np.zeros((X_test.shape[0], ntrees))\n",
    "\n",
    "for i in range(ntrees):\n",
    "    simpletree = DecisionTreeRegressor(max_depth=57)\n",
    "    boot_x, boot_y = resample(X_train[['logminority']], y_train)\n",
    "    estimators = np.append(estimators,simpletree.fit(boot_x, boot_y))\n",
    "    R2s = np.append(R2s,simpletree.score(X_test[['logminority']], y_test))\n",
    "    yhats_test[:,i] = simpletree.predict(X_test[['logminority']])\n",
    "\n",
    "\n",
    "######\n",
    "# your code here: \n",
    "# let's look at how the bagging does\n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Random Forests\n",
    "\n",
    "What's the basic idea?\n",
    "\n",
    "Bagging alone is not enough randomization, because even after bootstrapping, we are mainly training on the same data points using the same variables, and will retain much of the overfitting.\n",
    "\n",
    "So we will build each tree by splitting on \"random\" subset of predictors at each split (hence, each is a 'random tree').  This can't be done in with just one predcitor, but with more predictors we can choose what predictors to split on randomly and how many to do this on.  Then we combine many 'random trees' together by averaging their predictions, and this gets us a forest of random trees: a **random forest**.\n",
    "\n",
    "Here are the parameters we can play with:\n",
    "\n",
    "```\n",
    "max_features : int, float, string or None, optional (default=”auto”)\n",
    "- The number of features to consider when looking for the best split.\n",
    "```\n",
    "\n",
    "- `max_features`: Default splits on all the features and is probably prone to overfitting. You'll want to validate on this. \n",
    "- You can \"validate\" on the trees `n_estimators` as well but many times you will just look for the plateau in the trees as seen below.\n",
    "- From decision trees you get the `max_depth`, `min_samples_split`, and `min_samples_leaf` as well, but you can leave those at defaults to get a maximally expanded tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by fitting a single attempt at a random forest, with `n_estimators` = 50, `max_features` = 0.33, and `max_depth` = 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf1 = RandomForestRegressor(oob_score=True, n_estimators=100, max_features=0.33, max_depth=20)\n",
    "rf1.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Q3.1** What is the predicted votergap for 'Middlesex County' in 'Massachusetts' (in the training set) for this random forest model?  Determine $R^2$ in the test set for this random forest model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#this will be helpful\n",
    "X_train[(elect_train['state']=='Massachusetts') & (elect_train['county']=='Middlesex County')]\n",
    "\n",
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Q3.2** Play around with the 3 hyperparameters to try to improve the predictive ability of the random forest (call it `rf2`).  How much has this improved the $R^2$ in the test set?  What parameters did you settle on?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's automate this process: below we create a hyper-param Grid. We are preparing to use the bootstrap points not used in training for validation rather than having to 'lose observations' in train by doing standard cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from \n",
    "# Adventures in scikit-learn's Random Forest by Gregory Saunders\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "param_dict = OrderedDict(\n",
    "    n_estimators = [100, 300, 500],\n",
    "    max_features = [0.4, 0.5, 0.6, 0.7, 0.8]\n",
    ")\n",
    "\n",
    "param_dict.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the OOB score.\n",
    "\n",
    "We have been putting \"validate\" in quotes. This is because the bootstrap gives us left-over points! So we'll now engage in our very own version of a grid-search, done over the out-of-bag scores that `sklearn` gives us for free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure y_train is the correct data type...in case you have warnings\n",
    "# print(y_train.shape,X_train.shape)\n",
    "# y_train = np.ravel(y_train)\n",
    "# note: n_jobs=-1` parallelizes the process to use all your CPU threads\n",
    "\n",
    "#Let's Cross-val. on the two 'hyperparameters' we based our grid on earlier\n",
    "results = {}\n",
    "estimators= {}\n",
    "for ntrees, maxf in product(*param_dict.values()):\n",
    "    params = (ntrees, maxf)\n",
    "    est = RandomForestRegressor(oob_score=True, \n",
    "                                n_estimators=ntrees, max_features=maxf, max_depth=50, n_jobs=-1)\n",
    "    est.fit(X_train, y_train)\n",
    "    results[params] = est.oob_score_\n",
    "    estimators[params] = est\n",
    "outparams = max(results, key = results.get)\n",
    "outparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = estimators[outparams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can find the **feature importance** of each predictor in this random forest model. Whenever a feature is used in a tree in the forest, the algorithm will log the decrease in the splitting criterion (such as gini). This is accumulated over all trees and reported in `est.feature_importances_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rf_best.feature_importances_,index=list(X_train)).sort_values().plot(kind=\"barh\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our response isn't very symmetric, we may want to suppress outliers by using the `mean_absolute_error` instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_test, rf_best.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.3** Tune the 'RandomForestRegressor' above to minimize 'mean_absolute_error' instead of the default ____________\n",
    "\n",
    "*Note: `sklearn` supports this (`criterion='mae'`), but does not have completely arbitrary loss functions for Random Forests.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model would be expected to perform better in the test set when measuring mean absolute error, but not as good at mean squared error (but this is not guaranteed).  Here, we do not cross-validate, so that might explain why mean absolute error is worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: instead of using oob scoring, we could do cross-validation (with GridSearchCV), and a cv of 3 will roughly be comparable (same approximate train-vs.-validation set sizes). But this will take much more time as you are doing the fit 3 times plus the bootstraps. So at least three times as long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict2 = OrderedDict(\n",
    "    n_estimators = [400,600,800],\n",
    "    max_features = [0.2, 0.4, 0.6, 0.8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "est2 = RandomForestRegressor(oob_score=False)\n",
    "gs = GridSearchCV(est2, param_grid = param_dict2, cv=3, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = gs.best_estimator_\n",
    "rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.4** What are the 3 *hyperparameters* for a random forest (one of the hyperparameters come in many *flavors*)?  Which hyperparameters need to be tuned? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeing error as a function of the proportion of predictors at each split\n",
    "\n",
    "Let's look to see how `max_features` affects performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from http://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html\n",
    "\n",
    "feats = param_dict['max_features']\n",
    "# \n",
    "error_rate = OrderedDict((label, []) for label in feats)\n",
    "\n",
    "# Range of `n_estimators` values to explore.\n",
    "min_estimators = 50\n",
    "step_estimators = 50\n",
    "num_steps = 6\n",
    "max_estimators = min_estimators + step_estimators*num_steps\n",
    "for label in feats:\n",
    "    for i in range(min_estimators, max_estimators+1, step_estimators):\n",
    "        clf = RandomForestRegressor(oob_score=True, max_features=label)\n",
    "        clf.set_params(n_estimators=i)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Record the OOB error for each `n_estimators=i` setting.\n",
    "        oob_error = 1 - clf.oob_score_\n",
    "        error_rate[label].append((i, oob_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the \"OOB error rate\" vs. \"n_estimators\" plot.\n",
    "for label, clf_err in error_rate.items():\n",
    "    xs, ys = zip(*clf_err)\n",
    "    plt.plot(xs, ys, label=label)\n",
    "\n",
    "plt.xlim(min_estimators, max_estimators)\n",
    "plt.xlabel(\"n_estimators\")\n",
    "plt.ylabel(\"OOB error rate\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
