{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2018-CS109A/blob/master/content/styles/iacs.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 11: NNs and Visualizating Prediction Models\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2020**<br>\n",
    "**Instructors:** Kevin Rader<br>\n",
    "**Authors:** Rahul Dave, David Sondak, Pavlos Protopapas, Chris Tanner, Eleni Kaxiras, Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Review of Tree-based Models </li>     \n",
    "<li> Architecture of Artificial Neural Networks (ANNs) </li>     \n",
    "<li> Variable Importances </li> \n",
    "<li> Interpreting Prediction Models </li> \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "\n",
    "# Here are the decision trees\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)  # You should see a 2.0.0 here!\n",
    "\n",
    "# sns.set(style=\"ticks\")\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 11. By the end of this lecture, you should be able to:\n",
    "\n",
    "- have a better grasp of neural network archetecture\n",
    "- interpret a few different types of variable importances\n",
    "- interpret a prediction model by exploring the relationships of predictors with the response through prediction plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Data Wrangling\n",
    "\n",
    "For this notebook we will be using the heart data set we've used all semester for performing classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv('../data/Heart.csv')\n",
    "print(heart_df.shape)\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X and y\n",
    "X = heart_df[['Age','Sex','ChestPain','RestBP','Chol','Fbs','RestECG','MaxHR','ExAng','Oldpeak','Slope','Ca','Thal']]\n",
    "y = 1*(heart_df['AHD']=='Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix categorical data types for maching learning methods...don't worry about the warning message\n",
    "\n",
    "X['ChestPain']=X['ChestPain'].astype('category')\n",
    "X['ChestPain']=X['ChestPain'].cat.codes\n",
    "\n",
    "X['Thal']=X['Thal'].astype('category')\n",
    "X['Thal']=X['Thal'].cat.codes\n",
    "\n",
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing zeroes for the missing values in `CA`\n",
    "\n",
    "X['Ca']=X['Ca'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "itrain, itest = train_test_split(range(X.shape[0]), train_size=0.80)\n",
    "\n",
    "X_train = X.iloc[itrain, :]\n",
    "X_test = X.iloc[itest, :]\n",
    "y_train = y.iloc[itrain]\n",
    "y_test = y.iloc[itest]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: tree-based models\n",
    "\n",
    "Below `max_depth=3` and `max_depth=10` decision trees are fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the simple (depth = 3) decision tree classifier\n",
    "dt3= tree.DecisionTreeClassifier(max_depth = 3)\n",
    "dt3.fit(X_train,y_train)\n",
    "\n",
    "#fit the an overfit (depth = 10) decision tree classifier\n",
    "dt10 = tree.DecisionTreeClassifier(max_depth = 10)\n",
    "dt10.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1** Calculate the AUC on both train and test, and interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######\n",
    "#n Your code here\n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue fitting tree based models: first with a random forest, and then a boosted tree model.  Note: these are untuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(109)\n",
    "randomforest = RandomForestClassifier(n_estimators=100, max_features='sqrt', max_depth=10)\n",
    "randomforest.fit(X_train,y_train);\n",
    "\n",
    "adaboost = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=4),\n",
    "    n_estimators=500,\n",
    "    learning_rate=.75)\n",
    "adaboost.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating\n",
    "print(\"AUC on train for randomforest:\",sk.metrics.roc_auc_score(y_train,randomforest.predict_proba(X_train)[:,1]))\n",
    "print(\"AUC on test for randomforest:\",sk.metrics.roc_auc_score(y_test,randomforest.predict_proba(X_test)[:,1]))\n",
    "\n",
    "print(\"AUC on train for adaboost:\",sk.metrics.roc_auc_score(y_train,adaboost.predict_proba(X_train)[:,1]))\n",
    "print(\"AUC on test for adaboost:\",sk.metrics.roc_auc_score(y_test,adaboost.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2** What would happen to the above AUC on train and test (random forest and adaboost) if the number of estimators (base trees) were increased for each?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: NN model\n",
    "\n",
    "Below we build our first NN model for these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(100, input_shape=(pd.DataFrame(X_train).shape[1],), activation='relu'),\n",
    "    tf.keras.layers.Dense(25, activation='tanh'),\n",
    "    tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1** How many hidden layers does this model have?  What should be the loss function for this model?  What is incorrect in the model architecture above?  Be sure to fix it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fit the model, and evaluate:\n",
    "\n",
    "model_NN.compile(optimizer='ADAM', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model_NN.fit(X_train, y_train, epochs=100, batch_size=64, verbose=0)\n",
    "\n",
    "print(\"AUC on train for NN_model:\",sk.metrics.roc_auc_score(y_train,model_NN.predict_proba(X_train)))\n",
    "print(\"AUC on test for NN_model:\",sk.metrics.roc_auc_score(y_test,model_NN.predict_proba(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** Create a new NN model called `model_NN2` that improves upon the fixed model above.  Why do you suppose it is doing a better job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Variable Importance\n",
    "\n",
    "Below the variable importances are created for the 4 tree-based models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default Variable Importance\n",
    "\n",
    "plt.figure(figsize=(24,6))\n",
    "#plt.set_xticks()\n",
    "#plt.set_xticklabels(X.columns)\n",
    "num=10 \n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "dt3_importances = dt3.feature_importances_\n",
    "order = np.flip(np.argsort(dt3_importances))[0:num]\n",
    "plt.barh(range(num),dt3_importances[order],tick_label=X.columns[order]);\n",
    "plt.title(\"Relative Variable Importance for dt3\")\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "dt10_importances = dt10.feature_importances_\n",
    "order = np.flip(np.argsort(dt10_importances))[0:num]\n",
    "plt.barh(range(num),dt10_importances[order],tick_label=X.columns[order]);\n",
    "plt.title(\"Relative Variable Importance for dt10\")\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "rf_importances = randomforest.feature_importances_\n",
    "order = np.flip(np.argsort(rf_importances))[0:num]\n",
    "plt.barh(range(num),rf_importances[order],tick_label=X.columns[order]);\n",
    "plt.title(\"Relative Variable Importance for rf\")\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "adaboost_importances = adaboost.feature_importances_\n",
    "order = np.flip(np.argsort(adaboost_importances))[0:num]\n",
    "plt.barh(range(num),adaboost_importances[order],tick_label=X.columns[order]);\n",
    "plt.title(\"Relative Variable Importance for adaboost\");\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1** Interpret the plots above: why do they make sense?  How would the random forest variable imporance change if `max_features` was altered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use the [`eli5`](https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance) package to perform permutation importance for the random forest model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install eli5\n",
    "#permutation importance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "\n",
    "\n",
    "perm = PermutationImportance(randomforest).fit(X_test, y_test)\n",
    "#eli5.show_weights(perm,feature_names=X.columns)\n",
    "print(X.columns)\n",
    "eli5.show_weights(perm, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2** How do the permutation importances compare to the default feature importance?  What is the difference in interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: eli5 does not behave well with Keras, by default.\n",
    "\n",
    "perm = PermutationImportance(model_NN, random_state=1).fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Plotting Predictions\n",
    "\n",
    "\n",
    "Below we start to interpret relationships from various models based on the predictions from those models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_rf_train = randomforest.predict_proba(X_train)[:,1]\n",
    "plt.scatter(X_train[['Age']],yhat_rf_train);\n",
    "yhat_rf_test = randomforest.predict_proba(X_test)[:,1]\n",
    "plt.scatter(X_test[['Age']],yhat_rf_test,marker='x');\n",
    "plt.title(\"Predicted Probabilities vs. Age from the RF in train and test\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1** What does the above plot showing?  How can it be interpreted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1** Reproduce the above plot for your neural netowrk model.  How does it compare?  What does it say about Age's relationship with Cardiac Arrest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# Your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.3** Fit a logistic regression to the predicted response from your NN model based on Age (in train).  Interpret the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a few different plots are created:\n",
    "1. The predicted probabilities vs. age for any reasonable value of age at the mean values for the other predictors\n",
    "2. The predicted probabilties for each individual vs. Age (sometimes called profile plots) and the averaged individual probabilities vs. Age.\n",
    "3. The median of these individual predcited probability curves, along with the middle 95% ranges at any particular value of Age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means1 = X_train.mean(axis = 0)\n",
    "#means1 =pd.Series(means)\n",
    "means_df = (means1.to_frame()).transpose()\n",
    "#df_repeated = pd.concat(means*3)\n",
    "#print(df_repeated)\n",
    "Ages = np.arange(np.min(X['Age']),np.max(X['Age']))\n",
    "means_df  = pd.concat([means_df]*Ages.size,ignore_index=True)\n",
    "means_df['Age'] = Ages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots at means\n",
    "yhat_nn = NN_model.predict_proba(means_df)\n",
    "plt.scatter(X_train['Age'],y_train)\n",
    "plt.plot(means_df['Age'],yhat_nn,color=\"red\")\n",
    "plt.title(\"Predicted Probabilities vs. Age from NN in train\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots for all observations.  And then averaged\n",
    "\n",
    "means1 = X_train.mean(axis = 0)\n",
    "#means1 =pd.Series(means)\n",
    "means_df = (means1.to_frame()).transpose()\n",
    "#df_repeated = pd.concat(means*3)\n",
    "#print(df_repeated)\n",
    "Ages = np.arange(np.min(X['Age']),np.max(X['Age']))\n",
    "means_df  = pd.concat([means_df]*Ages.size,ignore_index=True)\n",
    "means_df['Age'] = Ages\n",
    "yhat_nns = []\n",
    "for i in range(0,X_train.shape[0]):\n",
    "    obs = X_train.iloc[i,:].to_frame().transpose()\n",
    "    obs_df  = pd.concat([obs]*Ages.size,ignore_index=True)\n",
    "    obs_df['Age'] = Ages\n",
    "    yhat_nn = NN_model.predict_proba(obs_df)\n",
    "    yhat_nns.append(yhat_nn.transpose())\n",
    "    plt.plot(obs_df['Age'],yhat_nn,color='blue',alpha=0.05)\n",
    "\n",
    "plt.plot(obs_df['Age'],np.mean(yhat_nns,axis=0)[0],color='red',linewidth=2);\n",
    "    \n",
    "plt.ylim(0,1)\n",
    "plt.title(\"Predicted Probabilities vs. Age from NN in train for all observations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(obs_df['Age'],np.median(yhat_nns,axis=0)[0],color='red');\n",
    "plt.plot(obs_df['Age'],np.quantile(yhat_nns,q=.05,axis=0)[0],color='blue');\n",
    "plt.plot(obs_df['Age'],np.quantile(yhat_nns,q=.95,axis=0)[0],color='blue');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.4** Interpret these plots.  What does the NN model say about the relationship between age and chances of cardiac arrest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.5** Why is it important to consider plotting for separate individuals rather than just doing the predictions at the mean value for the other predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
