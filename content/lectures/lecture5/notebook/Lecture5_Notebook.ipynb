{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2018-CS109A/blob/master/content/styles/iacs.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 5: Cross-Validation and Regularization\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2020**<br>\n",
    "**Instructors:** Kevin Rader<br>\n",
    "**Authors:** Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas, Chris Tanner, Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Cross-Validation</li>\n",
    "<li> Regulatization</li>\n",
    "<li> </li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 5. By the end of this lecture, you should be able to:\n",
    "\n",
    "- Know how to manually perform random subsets cross-validation\n",
    "- Perform $k$-fold cross-v alidation both manually and using provided libraries\n",
    "- Perform both forms of regularization (Ridge and LASSO) and understand the most basic use differences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels as sm\n",
    "import statsmodels.regression.linear_model as lm\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Reading the data \n",
    "\n",
    "We will be using the same data as last week (both the train and test splits): modeling `votergap` from the 2016 election (Trump-Clinton) from many predictors in the `county_election` dataset.\n",
    "\n",
    "We start by reading in the datasets for you and refitting some regression models from last week:\n",
    "\n",
    "**Important note: use the training dataset for all exploratory analysis and model fitting.  Only use the test dataset to evaluate and compare models.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinrader/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv(\"../data/county_election_train.csv\")\n",
    "test = pd.read_csv(\"../data/county_election_test.csv\")\n",
    "\n",
    "#recall we log-transformed several of the variables and did a little imputation in cancer variable (with the median)\n",
    "train['log_density'] = np.float64(np.log(train['density']))\n",
    "train['log_minority'] = np.float64(np.log(train['minority']))\n",
    "train['log_population'] = np.log(train['population'])\n",
    "train['log_hispanic'] = np.log(train['hispanic'])\n",
    "\n",
    "test['log_density'] = np.float64(np.log(test['density']))\n",
    "test['log_minority'] = np.float64(np.log(test['minority']))\n",
    "test['log_population'] = np.log(test['population'])\n",
    "test['log_hispanic'] = np.log(test['hispanic'])\n",
    "\n",
    "# imputing median cancer rate for the 40 or so counties with missing cancer rates\n",
    "train['cancer'].loc[train['cancer'].isnull()] = np.median(train['cancer'])\n",
    "test['cancer'].loc[test['cancer'].isnull()] = np.median(train['cancer'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q0.1** How were missing values imputed for the variable `cancer` in the test set?  Why was this choice made?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on the test set for the main effects model: 0.5752558339850569\n"
     ]
    }
   ],
   "source": [
    "# and we fit the simple regression model for y = votergap and x = log_density in both sklearn and statmodels\n",
    "\n",
    "predictors = ['log_population','log_hispanic', 'log_minority', 'female', \n",
    "             'unemployed', 'income', 'nodegree', 'bachelor', 'inactivity', 'obesity', 'log_density']\n",
    "X_train = train[predictors]\n",
    "X_test = test[predictors]\n",
    "\n",
    "regress_maineffects = LinearRegression(fit_intercept=True).fit(X_train, train['votergap'])\n",
    "\n",
    "r2_test_maineffects = sk.metrics.r2_score(test['votergap'], regress_maineffects.predict(X_test))\n",
    "print(\"R-squared on the test set for the main effects model:\", r2_test_maineffects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 66 interaction terms in the design matrix for this model.\n",
      "R-squared on the test set for the full interaction model: 0.7121772296069138\n"
     ]
    }
   ],
   "source": [
    "# And then we fit a model to \n",
    "\n",
    "X_interact = PolynomialFeatures(2, interaction_only=False, include_bias=True).fit_transform(train[predictors])\n",
    "print(\"There are\", X_interact.shape[1]-train[predictors].shape[1]-1,\"interaction terms in the design matrix for this model.\")\n",
    "\n",
    "regress_interact = LinearRegression(fit_intercept=False).fit(X_interact, train['votergap'])\n",
    "\n",
    "r2_test_interact = sk.metrics.r2_score(test['votergap'], regress_interact.predict(\n",
    "    PolynomialFeatures(2, interaction_only=False, include_bias=True).fit_transform(test[predictors])))\n",
    "print(\"R-squared on the test set for the full interaction model:\", r2_test_interact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor#: 65 with associated p-value of 0.9809439044018047\n",
      "Predictor#: 59 with associated p-value of 0.9305643717295858\n",
      "Predictor#: 30 with associated p-value of 0.9249852513988832\n",
      "Predictor#: 46 with associated p-value of 0.9084284356381296\n",
      "Predictor#: 5 with associated p-value of 0.9918462081475021\n",
      "Predictor#: 72 with associated p-value of 0.9333415365189452\n",
      "Predictor#: 71 with associated p-value of 0.9786271608656434\n",
      "Predictor#: 70 with associated p-value of 0.9804924564077684\n",
      "Predictor#: 20 with associated p-value of 0.9937354307652169\n",
      "Predictor#: 68 with associated p-value of 0.8927774675943259\n",
      "Predictor#: 26 with associated p-value of 0.9332163015373882\n",
      "Predictor#: 6 with associated p-value of 0.9414333774028818\n",
      "Predictor#: 37 with associated p-value of 0.7419127223057711\n",
      "Predictor#: 26 with associated p-value of 0.9287976413492167\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'[26] not found in axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0e48ff018875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predictor#:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"with associated p-value of\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdf_backwards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_backwards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_interact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_temp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3997\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3998\u001b[0m         )\n\u001b[1;32m   3999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3934\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3935\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3936\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3970\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5016\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5017\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5018\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5020\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[26] not found in axis'"
     ]
    }
   ],
   "source": [
    "# And lastly we did variable selection on this full model with interaction (backwards sequential variable selection)\n",
    "\n",
    "df_backwards = pd.DataFrame(X_interact)\n",
    "cutoff = 0.2\n",
    "\n",
    "for i in np.arange(df_backwards.shape[1]):\n",
    "    model_temp = lm.OLS(train['votergap'],df_backwards).fit()\n",
    "    if(np.max(model_temp.pvalues) > cutoff):\n",
    "        print(\"Predictor#:\", np.argmax(model_temp.pvalues), \"with associated p-value of\" ,np.max(model_temp.pvalues))\n",
    "        df_backwards = df_backwards.drop(pd.DataFrame(X_interact).columns[np.argmax(model_temp.pvalues)],axis=1)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "regress_backwards = LinearRegression(fit_intercept=False).fit(df_backwards, train['votergap'])\n",
    "df_backwards.shape\n",
    "\n",
    "X_interact_test = PolynomialFeatures(2, interaction_only=False, include_bias=True).fit_transform(test[predictors])\n",
    "X_backwards_test = pd.DataFrame(X_interact_test)[df_backwards.columns]\n",
    "\n",
    "r2_test_backwards = sk.metrics.r2_score(test['votergap'], regress_backwards.predict(X_backwards_test))\n",
    "print(\"R-squared on the test set for the full interaction model:\", r2_test_backwards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q0.2** Play around with the critical p-value cut-off for keeping variables.  What does this say about using 0.05 as the cut-off?  Will this generalize to all situations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Cross-Validation\n",
    "\n",
    "3 models were built above, and a single test-set was used to determine which was best for out-of-sample prediction.  Let's instead use cross-validation to decide which of the 3 models is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.RandomState(109)\n",
    "\n",
    "n_folds = 10\n",
    "k_fold = KFold(n_folds, random_state=109, shuffle=True)\n",
    "y_train = train['votergap']\n",
    "X_backwards = df_backwards\n",
    "X_interact = pd.DataFrame(X_interact)\n",
    "\n",
    "r2_cv_maineffects = []\n",
    "r2_cv_interact = []\n",
    "r2_cv_backwards = []\n",
    "\n",
    "for traincv_index, testcv_index in k_fold.split(X_train):\n",
    "    y_traincv, y_testcv = y_train[traincv_index], y_train[testcv_index]\n",
    "        \n",
    "    X_maineffects_traincv, X_maineffects_testcv = X_train.loc[traincv_index], X_train.loc[testcv_index]\n",
    "    X_interact_traincv, X_interact_testcv = X_interact.loc[traincv_index], X_interact.loc[testcv_index]\n",
    "    X_backwards_traincv, X_backwards_testcv = X_backwards.loc[traincv_index], X_backwards.loc[testcv_index]\n",
    "\n",
    "    model1 = LinearRegression(fit_intercept=True).fit(X_maineffects_traincv, y_traincv)\n",
    "    model2 = LinearRegression(fit_intercept=True).fit(X_interact_traincv, y_traincv)\n",
    "    model3 = LinearRegression(fit_intercept=True).fit(X_backwards_traincv, y_traincv)\n",
    "\n",
    "    r2_cv_maineffects.append(sk.metrics.r2_score(y_testcv, model1.predict(X_maineffects_testcv)))\n",
    "    r2_cv_interact.append(sk.metrics.r2_score(y_testcv, model2.predict(X_interact_testcv)))\n",
    "    r2_cv_backwards.append(sk.metrics.r2_score(y_testcv, model3.predict(X_backwards_testcv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1** Evaluate which of the 3 models considered above are best for out-of-sample prediction (both numerically and visually).  Does this agree with using the original test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573798312505083\n",
      "0.7210498926721767\n",
      "0.7284726301600148\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2** Play around with $k$ in the $k$-fold cross-validation above.  Does the choice of the number of folds really matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3** Build your own bootstrap cross-validation approach below (a skeleton is provided).  Do you see the same result comparing these 3 models?  What are the advantages to $k$-fold and random subsets/bootstrap cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nboots = 100\n",
    "n_train = train.shape[0]\n",
    "traincv_size = 0.8\n",
    "n_traincv = int(traincv_size*n_train)\n",
    "r2_cv_maineffects = []\n",
    "r2_cv_interact = []\n",
    "r2_cv_backwards = []\n",
    "\n",
    "\n",
    "for boot in np.arange(nboots):\n",
    "    traincv_indices = np.random.choice(n_train,n_traincv,replace=False)\n",
    "    \n",
    "    y_traincv, y_testcv = y_train[traincv_index], y_train[testcv_index]\n",
    "        \n",
    "    X_maineffects_traincv, X_maineffects_testcv = X_train.loc[traincv_index], X_train.loc[testcv_index]\n",
    "    X_interact_traincv, X_interact_testcv = X_interact.loc[traincv_index], X_interact.loc[testcv_index]\n",
    "    X_backwards_traincv, X_backwards_testcv = X_backwards.loc[traincv_index], X_backwards.loc[testcv_index]\n",
    "\n",
    "    model1 = LinearRegression(fit_intercept=True).fit(X_maineffects_traincv, y_traincv)\n",
    "    model2 = LinearRegression(fit_intercept=True).fit(X_interact_traincv, y_traincv)\n",
    "    model3 = LinearRegression(fit_intercept=True).fit(X_backwards_traincv, y_traincv)\n",
    "\n",
    "    r2_cv_maineffects.append(sk.metrics.r2_score(y_testcv, model1.predict(X_maineffects_testcv)))\n",
    "    r2_cv_interact.append(sk.metrics.r2_score(y_testcv, model2.predict(X_interact_testcv)))\n",
    "    r2_cv_backwards.append(sk.metrics.r2_score(y_testcv, model3.predict(X_backwards_testcv)))\n",
    " \n",
    "    ######\n",
    "    # your code here\n",
    "    ######\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-62a6e6ae247d>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-62a6e6ae247d>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    print(y_train[!1])\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "nboots = 100\n",
    "n_train = train.shape[0]\n",
    "traincv_size = 0.8\n",
    "n_traincv = int(traincv_size*n_train)\n",
    "r2_cv_maineffects = []\n",
    "r2_cv_interact = []\n",
    "r2_cv_backwards = []\n",
    "y_train = train['votergap']\n",
    "\n",
    "traincv_indices = np.random.choice(n_train,n_traincv,replace=False)\n",
    "print(y_train[1])\n",
    "print(y_train[!1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Data augmentation\n",
    "\n",
    "In this section, we will create two different desgin matrices that will be used going forward for applying reularization to.  The first is a high-order (10) polynomial regression using `log-minority` only, and the second is a high-dimensional design matrix considering all factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minority_poly = PolynomialFeatures(10, include_bias=False).fit_transform(train[['log_minority']])\n",
    "\n",
    "regress_minority_poly = LinearRegression(fit_intercept=True).fit(X_minority_poly , train['votergap'])\n",
    "print(\"Beta0 =\", regress_minority_poly.intercept_ ,\", Beta1 =\", regress_minority_poly.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1** Plot the predicted curves on top of the scatterplot of `votergap` vs. `log_minority` (the notebook from lecture 4 might be helpful for this).  What do you notice?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** Create a design matrix (call it `X_over`) with all the interactions and 4th-order polynomials based on the original list of 11 predictors: \n",
    "\n",
    "predictors = ['log_population','log_hispanic', 'log_minority', 'female', \n",
    "             'unemployed', 'income', 'nodegree', 'bachelor', 'inactivity', 'obesity', 'log_density']\n",
    "\n",
    "How many predictors does this create?  Justify this number mathematically (it is not easy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here: create X_over \n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3** Fit the linear model using the over-parameterized design matrix above.  How does it perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Regularization: Ridge and LASSO\n",
    "\n",
    "In this part we will explore the effects of regularization on linear regression modeling.  First using the 20th order polynomial version of log_minority, then using the over-parameterized design matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1** Fit a Ridge model with $\\alpha = 1$, $\\alpha = 10$, and a LASSO model with $\\alpha = 1$, and $\\alpha = 10$ using the `X_minority_poly` design matrix.  Compare the estimated coefficients for all 5 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we give the first one to you\n",
    "ridge_alpha1 = Ridge(alpha=1,fit_intercept=True).fit(X_minority_poly , train['votergap'])\n",
    "print(\"Beta0 =\", ridge_alpha1.intercept_ ,\", Beta1 =\", ridge_alpha1.coef_)\n",
    "\n",
    "\n",
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2**  Plot the predictions (on top of the scatterplot) for all 5 models.  What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.3** For Ridge, *tune* the penalty terms using the test set.  Which value did you choose? How does this perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ridge and LASSO with CV\n",
    "\n",
    "In this part we will use cross-validation to *tune* the penalization term ($\\alpha$) in both ridge and LASSO regressions for the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1** Use [ridgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) and [LassoCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) to tune the models using 5-fold cross-validation for the 10$^{th}$ order polynomial of `log_minority`.  You may need to play around with the values of $\\alpha$ you should consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.2** How do the best choices of $\\alpha$ compare for Ridge and Lasso?  Why is this not surprising, mathematically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.3** Why is it important to not include the bias term when creating the design matrices?  Why is it important to normalize the predictors?  Were these precautions taken here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.4** Fit a well-tuned regularization model on the overly parameterized design matrix `X_over`.  How does this perform on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
