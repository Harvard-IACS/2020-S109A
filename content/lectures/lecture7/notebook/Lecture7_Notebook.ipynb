{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2018-CS109A/blob/master/content/styles/iacs.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 7: $k$-NN Classification, Missingness, and PCA\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2020**<br>\n",
    "**Instructors:** Kevin Rader<br>\n",
    "**Authors:** Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas, Chris Tanner, Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> $k$-NN Classification </li> \n",
    "<li> PCA </li>\n",
    "<li> Missingness </li> \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 7. By the end of this lecture, you should be able to:\n",
    "\n",
    "- Fit, plot, and 'interpret' $k$-NN classification models\n",
    "- Determine classification boundaries (through plotting predictions) for $k$-NN models\n",
    "- Perform principal components analysis (PCA) on a set of predictors\n",
    "- Use the PCA vectors as the basis of modeling and visualizations\n",
    "- Understand the differences between missing types\n",
    "- Use basic imputation methods to handle missingness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.metrics as met\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Reading the data \n",
    "\n",
    "In this notebook, we will be using the same Heart dataset from last lecture.  As a reminder the variables we will be using today include:\n",
    "\n",
    "- `AHD`: whether or not the patient presents atherosclerotic heart disease (a heart attack): `Yes` or `No`\n",
    "- `Sex`: a binary indicator for whether the patient is male (Sex=1) or female (Sex=0)\n",
    "- `Age`: age of patient, in years\n",
    "- `MaxHR`: the maximum heart rate of patient based on exercise testing\n",
    "- `RestBP`: the resting systolic blood pressure of the patient\n",
    "- `Chol`: the HDL cholesterol level of the patient\n",
    "\n",
    "For further information on the dataset, please see the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_heart = pd.read_csv('../data/Heart.csv')\n",
    "\n",
    "# Force the response into a binary indicator:\n",
    "df_heart['AHD'] = 1*(df_heart['AHD'] == \"Yes\")\n",
    "\n",
    "print(df_heart.shape)\n",
    "df_heart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some basic summaries and EDA from last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heart.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_heart['Sex'],df_heart['AHD'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_heart['Thal'],df_heart['AHD'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_heart['ChestPain'],df_heart['AHD'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_heart['Age'][df_heart['AHD']==1])\n",
    "plt.hist(df_heart['Age'][df_heart['AHD']==0],alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_heart['MaxHR'][df_heart['AHD']==1])\n",
    "plt.hist(df_heart['MaxHR'][df_heart['AHD']==0],alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: $k$-NN Classification\n",
    "\n",
    "Several [*k*-nn classification models](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) using sklearn are fit and plotted below, using `MaxHR` as the sole predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = df_heart[['MaxHR']]\n",
    "data_y = df_heart['AHD']\n",
    "\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "knn10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn50 = KNeighborsClassifier(n_neighbors=50)\n",
    "knn150 = KNeighborsClassifier(n_neighbors=150)\n",
    "\n",
    "knn1.fit(data_x, data_y);\n",
    "knn10.fit(data_x, data_y);\n",
    "knn50.fit(data_x, data_y);\n",
    "knn150.fit(data_x, data_y);\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.patch.set_alpha(0.0)\n",
    "plt.xkcd(scale=0.1, length=0.0)\n",
    "plt.gcf().subplots_adjust(bottom=0.20, left = 0.16, right=0.86)\n",
    "\n",
    "\n",
    "x=np.linspace(np.min(data_x),np.max(data_x),500)\n",
    "yhat1 = knn1.predict(x)\n",
    "yhat10 = knn10.predict(x)\n",
    "yhat50 = knn50.predict(x)\n",
    "yhat150 = knn150.predict(x)\n",
    "\n",
    "plt.plot(data_x, data_y, 'o' ,alpha=0.1, label='Data')\n",
    "plt.plot(x,yhat1, label='knn1')\n",
    "plt.plot(x,yhat10, label='knn10')\n",
    "plt.plot(x,yhat50, label='knn50')\n",
    "plt.plot(x,yhat150, label='knn150')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"MaxHR\")\n",
    "plt.ylabel(\"Heart disease (AHD)\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1** Interpret these results: what is this plot showing?  How useful is it?  What would be a better plot to visualize the predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your asnwer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2** Create a similar plot as above, but instead use the predicted probabilities of success.  Interpret this plot: which model seems to be most appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-NN classification can also be applied to multiple predictors at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#two predictors\n",
    "\n",
    "data_x = df_heart[['MaxHR','Age']]\n",
    "\n",
    "knn1.fit(data_x, data_y);\n",
    "knn10.fit(data_x, data_y);\n",
    "knn50.fit(data_x, data_y);\n",
    "knn150.fit(data_x, data_y);\n",
    "\n",
    "print(knn1.score(data_x, data_y))\n",
    "print(knn10.score(data_x, data_y))\n",
    "print(knn50.score(data_x, data_y))\n",
    "print(knn150.score(data_x, data_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.3** Which predictor has more influence on the model above?  How could we fix this 'issue'?  Refit knn10 with the issue resolved (call it `knn10_fixed`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "######\n",
    "# your code here\n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot prediction on 2-D predictor space, some work needs to be done (several ways to do this).  This is done for you "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "x1=np.linspace(np.min(df_heart[['MaxHR']]),np.max(df_heart[['MaxHR']]),n)\n",
    "x2=np.linspace(np.min(df_heart[['Age']]),np.max(df_heart[['Age']]),n)\n",
    "x1v, x2v = np.meshgrid(x1, x2)\n",
    "\n",
    "# This is how we would typically do the prediction (have a vector of yhats)\n",
    "#yhat10 = knn10.predict(np.array([x1v.flatten(),x2v.flatten()]).reshape(-1,2))\n",
    "\n",
    "# To do the predictions and keep the yhats on 2-D (to match the dummy predictor shapes), use this\n",
    "yhat10 = knn10.predict(np.c_[x1v.ravel(), x2v.ravel()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(x1v.flatten(),x2v.flatten(),c=yhat10)\n",
    "plt.pcolormesh(x1v, x2v, yhat10.reshape(x1v.shape)) #, cmap=cmap_light\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.4** Recreate the plot above using `knn10_fixed`.  What differences do you notice?  How do these plots compare to what plot would look like from a logistic regression model?\n",
    "\n",
    "Hint: make sure you reapply your scaler on the X matrix when doing the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has a slightly different picture, but the same general pattern.  Note the logistic regression model (without interactions) would be a straight line.  See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to split into train and test  (or better yet, use cross-validation) \n",
    "# generally speaking to evaluate model performance or to determine what k is actually best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Principal Components Analysis (PCA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1** Just a sidebar (and a curiosity), what happens when two of the identical predictor is used in logistic regression?  Is an error created?  Should one be?  Investigate by predicting `AHD` from two copies of `Age`, and compare to the simple logistic regression model with `Age` alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_heart['AHD']\n",
    "\n",
    "logit1 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(df_heart[['Age']],y)\n",
    "\n",
    "# investigating what happens when two identical predictors are used\n",
    "\n",
    "######\n",
    "# your code here\n",
    "######\n",
    "\n",
    "\n",
    "print(\"The coef estimate for Age (when in the model once):\",logit1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply PCA to the heart dataset when there are just 4 predictors considered (remember: PCA is used when dimensionality is high (lots of predictors), but this will help us get our heads around what is going on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pedagogical purposes, let's simplify our lives and use just 4 predictors\n",
    "X = df_heart[['Age','RestBP','Chol','MaxHR']]\n",
    "y = df_heart['AHD']\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's fit the full logistic regression model to predict `AHD` from the 4 predictors above.\n",
    "\n",
    "Remember: PCA is an approach to handling the predictors, so it does not matter if we are using it for a regression or classification type problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the 'full' model on the 4 predictors. and print out the coefficients\n",
    "logit_full = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(X,y)\n",
    "\n",
    "beta = logit_full.coef_[0]\n",
    "\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** Is there any evidence of multicollinearity in the set of predictors?  How do you know?  How will PCA handle these correlations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply the [PCA transformation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) in a few steps, and show some of the results below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create/fit the 'full' pca transformation\n",
    "pca = PCA().fit(X)\n",
    "\n",
    "# apply the pca transformation to the full predictor set\n",
    "pcaX = pca.transform(X)\n",
    "\n",
    "# convert to a data frame\n",
    "pcaX_df = pd.DataFrame(pcaX, columns=[['PCA1' , 'PCA2', 'PCA3', 'PCA4']])\n",
    "\n",
    "# here are the weighting (eigen-vectors) of the variables (first 2 at least)\n",
    "print(\"First PCA Component (w1):\",pca.components_[0,:])\n",
    "print(\"Second PCA Component (w2):\",pca.components_[1,:])\n",
    "\n",
    "# here is the variance explained:\n",
    "print(\"Variance explained by each component:\",pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_.shape)\n",
    "print(pcaX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3** Interpret the results above.  What doss $w_1$ represent?  Why do the values make sense?  What does it's values squared sum up to?  Why does this make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common for a model with high dimensional data (lots of predictors) to be plotted along the first 2 PCA components (with the classification boundaries added).  Below is the scatter plot for these data (without a classificaiton boundary, since we do not have a model yet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the response over the first 2 PCA component vectors\n",
    "\n",
    "plt.scatter(pcaX_df[['PCA1']][y==0],pcaX_df[['PCA2']][y==0])\n",
    "plt.scatter(pcaX_df[['PCA1']][y==1],pcaX_df[['PCA2']][y==1])\n",
    "\n",
    "plt.legend([\"AHD = No\",\"AHD = Yes\"])\n",
    "plt.xlabel(\"First PCA Component Vector (Z1)\")\n",
    "plt.ylabel(\"Second PCA Component Vector (Z2)\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.4** What would a classification boundary look like if a logistic regression model were fit using the first 2 principal components as the predictors?  Does there appear to be good potential here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the result of the PCR-1 (logistic) to predict `AHD` from the first principal component vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_pcr1 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df[['PCA1']],y)\n",
    "\n",
    "print(\"Intercept from simple PCR-Logistic:\",logit_pcr1.intercept_)\n",
    "print(\"'Slope' from simple PCR-Logistic:\", logit_pcr1.coef_)\n",
    "\n",
    "print(\"First PCA Component (w1):\",pca.components_[0:1,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.5** What does this PCR-1 model tell us about how the predictors relate to the response (aka, estimate the coefficient(s) in the original predictor space)?  Is it truly a simple logistic regression model in the original predictor space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the above claculation for all 4 PCR logistic regressions, and then plotted on a pretty plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the other 3 PCRs on the rest of the 4 predictors\n",
    "\n",
    "logit_pcr2 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df[['PCA1','PCA2']],y)\n",
    "logit_pcr3 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df[['PCA1','PCA2','PCA3']],y)\n",
    "logit_pcr4 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df[['PCA1','PCA2','PCA3','PCA4']],y)\n",
    "\n",
    "pcr1=(logit_pcr1.coef_*np.transpose(pca.components_[0:1,:])).sum(axis=1)\n",
    "pcr2=(logit_pcr2.coef_*np.transpose(pca.components_[0:2,:])).sum(axis=1)\n",
    "pcr3=(logit_pcr3.coef_*np.transpose(pca.components_[0:3,:])).sum(axis=1)\n",
    "pcr4=(logit_pcr4.coef_*np.transpose(pca.components_[0:4,:])).sum(axis=1)\n",
    "\n",
    "results = np.vstack((pcr1,pcr2,pcr3,pcr4,beta))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(['PCR1' , 'PCR2', 'PCR3', 'PCR4', 'Logistic'],results)\n",
    "\n",
    "plt.ylabel(\"Back-calculated Beta Coefficients\");\n",
    "\n",
    "plt.legend(X.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.6** Interpret the plot above.  Specifically, compare how each PCA vector \"contributes\" to the original logistic regression model using all 4 original predictors.  How Does PCR-4 compare to the original logistic regression model (in estimated coefficients)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this PCA work should have been done using the standardized versions of the predictors.  Below is the code that does exactly that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sk.preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "Z = scaler.transform(X)\n",
    "pca = PCA(n_components=4).fit(Z)\n",
    "pcaZ = pca.transform(Z)\n",
    "pcaZ_df = pd.DataFrame(pcaZ, columns=[['PCA1' , 'PCA2', 'PCA3', 'PCA4']])\n",
    "\n",
    "print(\"First PCA Component (w1):\",pca.components_[0,:])\n",
    "print(\"Second PCA Component (w2):\",pca.components_[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the 'full' model on the 4 predictors. and print out the coefficients\n",
    "logit_full = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(Z,y)\n",
    "\n",
    "\n",
    "betaZ = logit_full.coef_[0]\n",
    "\n",
    "print(\"Logistic coef. on standardized predictors:\",betaZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the PCR\n",
    "logit_pcr1Z = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaZ_df[['PCA1']],y)\n",
    "logit_pcr2Z = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaZ_df[['PCA1','PCA2']],y)\n",
    "logit_pcr3Z = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaZ_df[['PCA1','PCA2','PCA3']],y)\n",
    "logit_pcr4Z = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaZ_df[['PCA1','PCA2','PCA3','PCA4']],y)\n",
    "\n",
    "pcr1Z=(logit_pcr1Z.coef_*np.transpose(pca.components_[0:1,:])).sum(axis=1)\n",
    "pcr2Z=(logit_pcr2Z.coef_*np.transpose(pca.components_[0:2,:])).sum(axis=1)\n",
    "pcr3Z=(logit_pcr3Z.coef_*np.transpose(pca.components_[0:3,:])).sum(axis=1)\n",
    "pcr4Z=(logit_pcr4Z.coef_*np.transpose(pca.components_[0:4,:])).sum(axis=1)\n",
    "\n",
    "resultsZ = np.vstack((pcr1Z,pcr2Z,pcr3Z,pcr4Z,betaZ))\n",
    "print(resultsZ)\n",
    "\n",
    "plt.plot(['PCR1-Z' , 'PCR2-Z', 'PCR3-Z', 'PCR4-Z', 'Logistic'],resultsZ)\n",
    "\n",
    "plt.ylabel(\"Back-calculated Beta Coefficients\");\n",
    "\n",
    "plt.legend(X.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.7** Compare this plot to the previous one; why does this plot make sense?.  What does this illustrate?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Dealing with Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some missing values to begin with\n",
    "print(df_heart.shape)\n",
    "print(df_heart.dropna().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1** Where are the missing values (in what variables)?  Do any subjects have multiple missing values?  How do you know?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "print(df_heart.isnull().sum())\n",
    "df_heart.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of cells crfeate missing values into `MaxHR` using 3 different techniques.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as random\n",
    "random.seed(109)\n",
    "n = df_heart[\"MaxHR\"].size\n",
    "\n",
    "# create 50 missing completely at random observations\n",
    "miss = random.choice(n,50)\n",
    "\n",
    "heart_mcar = pd.read_csv('../data/Heart.csv')\n",
    "heart_mcar.loc[miss,\"MaxHR\"] = np.nan\n",
    "print(heart_mcar[\"MaxHR\"][miss].head())\n",
    "print(heart_mcar.dropna().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create roughly 20% missing at random observations \n",
    "miss = random.binomial(1,0.1+0.2*df_heart[\"Sex\"],n)\n",
    "\n",
    "heart_mar = pd.read_csv('../data/Heart.csv')\n",
    "heart_mar.loc[miss==1,\"MaxHR\"]=np.nan\n",
    "print(heart_mar.loc[miss==1,\"MaxHR\"].head())\n",
    "print(heart_mar.dropna().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create about 20 missing not at random observations \n",
    "miss = random.binomial(1,0.2*(df_heart[\"MaxHR\"]>df_heart[\"MaxHR\"].mean()),n)\n",
    "\n",
    "heart_mnar = pd.read_csv('../data/Heart.csv')\n",
    "heart_mnar.loc[miss==1,\"MaxHR\"]=np.nan\n",
    "print(heart_mnar.loc[miss==1,\"MaxHR\"].head())\n",
    "print(heart_mnar.dropna().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2** Explain why these satisfy the conditions for the 3 types of missingness explained in the lecture.  How should they be treated in a data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an attempt to fit a model when missing values are present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn is not happy when you give it missing values\n",
    "knn50 = KNeighborsClassifier(n_neighbors=50)\n",
    "\n",
    "data_x = heart_mcar[['MaxHR','RestBP']]\n",
    "\n",
    "knn50.fit(data_x, data_y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So let's just fill in the mean to make it happy\n",
    "\n",
    "data_x = data_x.fillna(data_x.mean())\n",
    "\n",
    "knn50.fit(data_x, data_y);\n",
    "\n",
    "plt.hist(data_x['MaxHR'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.3** Rather than impute using the mean, perform 3 other types of imputation:\n",
    "\n",
    "1. Hot deck imputation.\n",
    "2. Impute using an appropriate model from the other predictors.\n",
    "3. Impute using an appropriate model (with stochastic error) from the other predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
