{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2018-CS109A/blob/master/content/styles/iacs.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 10: Boosting, Stacking, and ANNs\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2020**<br>\n",
    "**Instructors:** Kevin Rader<br>\n",
    "**Authors:** Rahul Dave, David Sondak, Pavlos Protopapas, Chris Tanner, Eleni Kaxiras, Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Quick Comparison of Methods </li> \n",
    "<li> Basic Boosting and AdaBoost </li> \n",
    "<li> Basics of Artificial Neural Networks (ANNs) </li> \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 10. By the end of this lecture, you should be able to:\n",
    "\n",
    "- understand the basic idea of boosting as an esemble method\n",
    "- use AdaBoost to fit a boosted model for a classification problem\n",
    "- build and understand a simple artifical neural network (ANN)\n",
    "- use Keras to build and fit complex ANNs automatically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "import sklearn.metrics as met\n",
    "\n",
    "\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Data Creation\n",
    "\n",
    "We will be using artificial data throughout this notebook for pedagogical reasons.  First we create some  classification response data based on pseudo-GPS coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create coordinates, then response based on those coordinates\n",
    "np.random.seed(109)\n",
    "data = np.random.multivariate_normal([0, 0], np.eye(2) * 5, size=1000)\n",
    "data = np.hstack((data, np.zeros((1000, 1))))\n",
    "data[data[:, 0]**2 + data[:, 1]**2 < 3**2, 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "x = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "ax.scatter(x[y == 1, 0], x[y == 1, 1], c='green', label='vegetation', alpha=0.5)\n",
    "ax.scatter(x[y == 0, 0], x[y == 0, 1], c='gray', label='non vegetation', alpha=0.1)\n",
    "ax.set_xlabel('longitude')\n",
    "ax.set_ylabel('latitude')\n",
    "ax.set_title('satellite image')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Comparing Boosting to other tree-based Models\n",
    "\n",
    "We give you a lot of code below to estimate 4 different tree-based models (none of which are tuned, for computational efficiency purposes).  Each has been recalculated for 5 separate re-creations of the data:\n",
    "\n",
    "- A single decision tree (depth = 10)\n",
    "- A Bagging model (with 20 base trees, default other parameters)\n",
    "- A Random Forest model (with 20 base trees, default other parameters)\n",
    "- A Boosted model (with 20 base trees each with a depth of 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot_dt(x, y, depth, title, ax, plot_data=True, fill=True, color='Greens'):\n",
    "    # FIT DECISION TREE MODEL\n",
    "    dt = tree.DecisionTreeClassifier(max_depth = depth)\n",
    "    dt.fit(x, y)\n",
    "\n",
    "    # PLOT DECISION TREE BOUNDARY\n",
    "    ax = plot_tree_boundary(x, y, dt, title, ax, plot_data, fill, color)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_tree_boundary(x, y, model, title, ax, plot_data=True, fill=True, color='Greens', alpha=0.1):\n",
    "    if plot_data:\n",
    "        # PLOT DATA\n",
    "        ax.scatter(x[y==1,0], x[y==1,1], c='green')\n",
    "        ax.scatter(x[y==0,0], x[y==0,1], c='white')\n",
    "    \n",
    "    # CREATE MESH\n",
    "    interval = np.arange(min(x.min(), y.min()),max(x.max(), y.max()),0.01)\n",
    "    n = np.size(interval)\n",
    "    x1, x2 = np.meshgrid(interval, interval)\n",
    "    x1 = x1.reshape(-1,1)\n",
    "    x2 = x2.reshape(-1,1)\n",
    "    xx = np.concatenate((x1, x2), axis=1)\n",
    "\n",
    "    # PREDICT ON MESH POINTS\n",
    "    yy = model.predict(xx)    \n",
    "    yy = yy.reshape((n, n))\n",
    "\n",
    "    # PLOT DECISION SURFACE\n",
    "    x1 = x1.reshape(n, n)\n",
    "    x2 = x2.reshape(n, n)\n",
    "    if fill:\n",
    "        ax.contourf(x1, x2, yy, alpha=alpha, cmap=color)\n",
    "    else:\n",
    "        ax.contour(x1, x2, yy, alpha=alpha, cmap=color)\n",
    "    \n",
    "    # LABEL AXIS, TITLE\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Latitude')\n",
    "    ax.set_ylabel('Longitude')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "#Variance reduction: Baggining, RF, Tree, Adaboost\n",
    "depth = 10\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    new_data = np.random.multivariate_normal([0, 0], np.eye(2) * 5, size=100)\n",
    "    new_data = np.hstack((new_data, np.zeros((100, 1))))\n",
    "    new_data[new_data[:, 0]**2 + new_data[:, 1]**2 < 3**2, 2] = 1\n",
    "    x = new_data[:, :-1]\n",
    "    y = new_data[:, -1]\n",
    "\n",
    "    ax[0] = fit_and_plot_dt(x, y, depth, 'Variance of Single Decision Tree', ax[0], plot_data=False, fill=False)\n",
    "    \n",
    "    bag = ensemble.BaggingClassifier(n_estimators=20)\n",
    "    bag.fit(x, y)\n",
    "    ax[1] = plot_tree_boundary(x, y, bag, 'Variance of Bagging', ax[1], plot_data=False, fill=False, color='Reds')\n",
    "    \n",
    "    rf = ensemble.RandomForestClassifier(n_estimators=20)\n",
    "    rf.fit(x, y)\n",
    "    ax[2] = plot_tree_boundary(x, y, rf, 'Variance of RF', ax[2], plot_data=False, fill=False, color='Blues')\n",
    "\n",
    "    adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=2), n_estimators=20)\n",
    "    adaboost.fit(x, y)\n",
    "    ax[3] = plot_tree_boundary(x, y, adaboost, 'Variance of AdaBoost', ax[3], plot_data=False, fill=False, color='Greys')\n",
    "    \n",
    "ax[0].set_xlim(-3.2, 3.2)\n",
    "ax[0].set_ylim(-3.2, 3.2)\n",
    "ax[1].set_xlim(-3.2, 3.2)\n",
    "ax[1].set_ylim(-3.2, 3.2)\n",
    "ax[2].set_xlim(-3.2, 3.2)\n",
    "ax[2].set_ylim(-3.2, 3.2)\n",
    "ax[3].set_xlim(-3.2, 3.2)\n",
    "ax[3].set_ylim(-3.2, 3.2)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1** Assess the 4 plots above.  Which type of model is best able to capture the true decision boundary (that is truly circular).  How could they each be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.2** Assess the computational time it takes for the random forest and bagging models above take to be fit (just use the data given below (note: we add more 'noise' to the data) and the hints for calculating computational time).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "new_data = np.random.multivariate_normal([0, 0], np.eye(2) * 5, size=n)\n",
    "new_data = np.hstack((new_data, np.zeros((n, 1))))\n",
    "new_data[new_data[:, 0]**2 + new_data[:, 1]**2 + np.random.normal(loc=0,scale=10,size=n) < 3**2, 2] = 1\n",
    "x = new_data[:, :-1]\n",
    "y = new_data[:, -1]\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.process_time() \n",
    "\n",
    "######\n",
    "# your code here for the random forest\n",
    "######\n",
    "\n",
    "\n",
    "rf_time_elapsed = time.process_time() - start_time\n",
    "\n",
    "print(\"Random Forest Took\",rf_time_elapsed,\"seconds.\")\n",
    "\n",
    "######\n",
    "# your code here for the adaboost model\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started using [`ensemble.AdaBoostClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), but this section shows how it behaves with changing the base tree depth and the number of trees used (using data similar to earlier, but with more 'noise').\n",
    "\n",
    "The code below calcaultes a boosted model using base trees of depth 2 (as above) for ensembling 100 trees together.  Then the cumulative predictive behavior is plotted as the depth increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.random.multivariate_normal([0, 0], np.eye(2) * 5, size=n)\n",
    "test_data = np.hstack((test_data, np.zeros((n, 1))))\n",
    "test_data[test_data[:, 0]**2 + test_data[:, 1]**2 + np.random.normal(loc=0,scale=10,size=n) < 3**2 , 2] = 1\n",
    "x_test = test_data[:, :-1]\n",
    "y_test = test_data[:, -1]\n",
    "\n",
    "n_trees = 100\n",
    "depth = 2\n",
    "learnrate = 1\n",
    "\n",
    "adaboost = ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=depth), n_estimators=n_trees, learning_rate=learnrate)\n",
    "adaboost.fit(x, y)\n",
    "\n",
    "acc_train = list(adaboost.staged_score(x,y))\n",
    "acc_test = list(adaboost.staged_score(x_test,y_test))\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(range(1,n_trees+1),acc_train,'*-',label=\"Train Accuracy\")\n",
    "plt.plot(range(1,n_trees+1),acc_test,'*-',label=\"Test Accuracy\")\n",
    "plt.ylim(0.5,1.02)\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1** What do you notice in the plot above? When does this boosted model start to overfit?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** What are the 3 parameters that can be tuned in a boosted model?  How would this boosted model predicted ability change if each of the 3 parameters were changed (one at a time)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3** Use the framework above to add a new boosted classifier to the plot.  Play around with the parameters of the boosted model to try to improve the predictive ability on the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "#####\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Basics of Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class='theme' > Keras and TENSORFLOW 2.0 </div>\n",
    "\n",
    "In this tutorial we will repeat what we have done in Lecture 18, exercise but now use keras as part of the tensorflow 2.0.\n",
    "\n",
    "TensorFlow is a framework for representing complicated DNN algorithms and executing them in any platform, from a phone to a distributed system using GPUs. Developed by Google Brain, TensorFlow is used very broadly today. \n",
    "\n",
    "**[`keras`](https://keras.io/)**, is a high-level API used for fast prototyping, advanced research, and production. We will use `tf.keras` which is TensorFlow's implementation of the `keras` API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Models are assemblies of layers\n",
    "\n",
    "The core data structure of Keras is a **model**, a way to organize layers. A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.\n",
    "\n",
    "The simplest type of model is the **Sequential** model, a linear stack of layers. For more complex architectures, one can use the Keras **Functional** API, which allows to build arbitrary graphs of layers.\n",
    "\n",
    "https://keras.io/models/model/\n",
    "\n",
    "Everything you need to know about the Sequential model is here: https://keras.io/models/sequential/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tensorflow  Installation\n",
    "Just a quick reminder, in Lecture 18 ex1 and ex2 we combine neurons and we adjusted the weights manually to produce a function that looks like a guassian. In this demo we will first do exactly the same as in the exercise but now use TF. At the end we will ask TF (nicely) to find the weights for us. How this is done behind the scenes will be explained in Lecture 20. For now we are only looking at the mechanics. \n",
    "\n",
    "https://www.tensorflow.org/install/\n",
    "\n",
    "pip install tensorflow=2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# usual imports \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import tensorflow \n",
    "import tensorflow as tf\n",
    " \n",
    "# If it is not installed yet, you can use this:\n",
    "# pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Instructions for running `tf.keras` with Tensorflow 2.0:  \n",
    "\n",
    "All references to Keras should be written as `tf.keras`.  For example: \n",
    "\n",
    "```\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "tf.keras.models.Sequential\n",
    "tf.keras.layers.Dense, tf.keras.layers.Activation, \n",
    "tf.keras.layers.Dropout, tf.keras.layers.Flatten, tf.keras.layers.Reshape\n",
    "tf.keras.optimizers.SGD\n",
    "tf.keras.preprocessing.image.ImageDataGenerator\n",
    "tf.keras.regularizers\n",
    "tf.keras.datasets.mnist   \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# You can avoid the long names by using\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#check if the correct version of tensorflow is used\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation\n",
    "We will be using artificial data for this part of the lab (for pedagogical reasons).  Note: we can always create more data to build a test set later).\n",
    "\n",
    "We create and visualize the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate data \n",
    "\n",
    "np.random.seed(109)\n",
    "x = np.linspace(-5,5,1000)\n",
    "y_real = np.exp( - (x**2)) + np.random.normal(loc=1, scale=0.2, size=x.shape)+0.00001*(x**6)\n",
    "plt.scatter(x,y_real);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First model: one neuro \n",
    "\n",
    "<img src='../fig/SingleNeuron.png'  height=\"442\" width=\"442\"></img>\n",
    "\n",
    "\n",
    "Build by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#reset the model \n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "# hidden layer (and output layer), one neuron \n",
    "model.add(layers.Dense(1, activation='sigmoid', input_shape=(1,)))\n",
    "\n",
    "weights = model.get_weights()\n",
    "# set the weights by hand as before\n",
    "weights[0][0]=1       # weight \n",
    "weights[1][0]=-0.4    # bias \n",
    "\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# check if the network is what we expected\n",
    "# Summary \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# predict \n",
    "y_oneneuron = model.predict(x)\n",
    "\n",
    "# plot \n",
    "plt.plot(x,y_oneneuron, label='one neuron model (sigmoid activation)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Two layers as \n",
    "<img src='../fig/multiple-perceptrons.png'  height=\"542\" width=\"542\"></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#reset the model \n",
    "model = models.Sequential()\n",
    "\n",
    "# hiddeb layer with 2 nodes \n",
    "model.add(layers.Dense(2, activation='sigmoid', input_shape=(1,)))\n",
    "\n",
    "\n",
    "# output layer, one neuron \n",
    "model.add(layers.Dense(1,  activation='linear'))\n",
    "\n",
    "#Weights \n",
    "weights = model.get_weights()\n",
    "# hidden layer\n",
    "weights[0][0]=np.array([ 1.5,-2.4]) #weights \n",
    "weights[1]=np.array([-0.4,2.0]) # biases\n",
    "# output layer \n",
    "weights[2]=np.array([[3],[3]]) # weights\n",
    "weights[3] = np.array([-2])    # bias\n",
    "\n",
    "model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Again check the network to make sure is what we expeected \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_twoneurons = model.predict(x)\n",
    "\n",
    "plt.scatter(x, y_real, alpha=0.1, label='data')\n",
    "plt.plot(x,y_twoneurons, label='two neuron model ()')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let Tensorflow do the job finding the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#reset the model \n",
    "model_tf = models.Sequential()\n",
    "\n",
    "# hidden layer with 2 nodes \n",
    "model_tf.add(layers.Dense(2, activation='sigmoid', input_shape=(1,)))\n",
    "\n",
    "\n",
    "# output layer, one neuron (note: it is a quantitative outpu)\n",
    "model_tf.add(layers.Dense(1,  activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model_tf.compile(loss='MSE')\n",
    "history = model_tf.fit(x,y_real, epochs=500, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_tf = model_tf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y_real, alpha=0.1, label='data')\n",
    "plt.plot(x,y_twoneurons, label='two neurons - ours')\n",
    "plt.plot(x,y_tf, label='tf')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "weights_tf = model_tf.get_weights()\n",
    "\n",
    "print(\"TensorFlow weights:\\n\", weights_tf)\n",
    "print()\n",
    "print(\"my weights:\\n\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1** Assess the accuracy of this quantitative $y$ variable in both the data used to train the model, and based on the newly created test datasets below (there are two of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1 = np.linspace(-5,5,1000)\n",
    "y_test1 = np.exp( - (x_test1**2)) + np.random.normal(loc=1, scale=0.2, size=x_test1.shape)+0.00001*(x_test1**6)\n",
    "x_test2 = np.linspace(-5,7,1000)\n",
    "y_test2 = np.exp( - (x_test2**2)) + np.random.normal(loc=1, scale=0.2, size=x_test2.shape)+0.00001*(x_test2**6)\n",
    "\n",
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** Recreate the scatterplot of all sets of data (train and both tests) along with the fitted prediction curve from `model_tf`.  What do you notice?  Why did the model perform poorly on the test set?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3** Create an overfit model using TensorFlow (simple to do: just increase the number of layers and/or the number of nodes within layers).  Play around with the activations in the interior layers as well).  Assess how it predicts and behaves on the test sets.  Have you beaten the models above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# Edit code below: this code creates the model above, in one go\n",
    "######\n",
    "\n",
    "model_tf2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(2, input_shape=(pd.DataFrame(x).shape[1],), activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation='linear'),\n",
    "])\n",
    "\n",
    "\n",
    "model_tf2.compile(loss='MSE')\n",
    "history = model_tf2.fit(x,y_real, epochs=500, batch_size=64, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
