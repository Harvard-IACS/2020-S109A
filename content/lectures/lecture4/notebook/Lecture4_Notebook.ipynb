{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://github.com/Harvard-IACS/2018-CS109A/blob/master/content/styles/iacs.png?raw=true\"> CS-S109A Introduction to Data Science \n",
    "\n",
    "## Lecture 4: Multiple Linear Regression\n",
    "\n",
    "**Harvard University**<br>\n",
    "**Summer 2020**<br>\n",
    "**Instructors:** Kevin Rader<br>\n",
    "**Authors:** Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas, Chris Tanner, Kevin Rader\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN THIS CELL TO GET THE RIGHT FORMATTING \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "<ol start=\"0\">\n",
    "<li> Learning Goals </li>\n",
    "<li> Bootstrap Resampling</li>\n",
    "<li> Multiple Linear Regression Basics</li>\n",
    "<li> Predictors, predictors, predictors </li>\n",
    "<li> Overfitting </li>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "This Jupyter notebook accompanies Lecture 4. By the end of this lecture, you should be able to:\n",
    "\n",
    "- Know the basic approach to implementing resampling methods (bootstrapping and randomization testing) in python.\n",
    "- Create *design matrices* to use for specific models to be fit.\n",
    "- Be comfortable fitting, interpreting and using multiple regression models from both `sklearn` and `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import statsmodels as sm\n",
    "import statsmodels.regression.linear_model as lm\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Reading the data and re-fitting Lecture 3 models\n",
    "\n",
    "We will be using the same data as last time (both the train and test splits): modeling `votergap` from the 2016 election (Trump-Clinton) from `density` (population density in persons per square mile) and other predictors where each row represents a county in the US. \n",
    "\n",
    "We start by reading in the datasets for you and refitting the simple regression models from last time:\n",
    "\n",
    "**Important note: use the training dataset for all exploratory analysis and model fitting.  Only use the test dataset to evaluate and compare models.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv(\"../data/county_election_train.csv\")\n",
    "test = pd.read_csv(\"../data/county_election_test.csv\")\n",
    "\n",
    "#recall we log-transformed population density to make linear regression more appropriate\n",
    "train['log_density'] = np.float64(np.log(train['density']))\n",
    "test['log_density'] = np.float64(np.log(test['density']))\n",
    "\n",
    "# and we fit the simple regression model for y = votergap and x = log_density in both sklearn and statmodels\n",
    "\n",
    "# sklearn: note the better way to define a single predictor design matrix\n",
    "regress1 = LinearRegression(fit_intercept=True).fit(train[['log_density']], train['votergap'])\n",
    "print(\"Beta0 =\", regress1.intercept_ ,\", Beta1 =\", regress1.coef_)\n",
    "\n",
    "# statsmodels approach: remember to manually add the intercept.\n",
    "X = sm.tools.add_constant(train['log_density'])\n",
    "model1 = lm.OLS(train['votergap'],X).fit()\n",
    "print(\"Statmodels results: \\n\",model1.params,sep=\"\")\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the line on the scatterplot\n",
    "\n",
    "x_dummy = np.arange(-2,11,0.1)\n",
    "yhat_dummy_regress = regress1.predict(x_dummy.reshape(-1, 1))\n",
    "\n",
    "plt.scatter(train['log_density'],train['votergap'])\n",
    "plt.plot(x_dummy,yhat_dummy_regress,c=\"r\",label=\"Linear Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally, evaluate on the test set (useful going forward):\n",
    "\n",
    "yhat1_train = regress1.predict(train[['log_density']])\n",
    "yhat1_test = regress1.predict(test[['log_density']])\n",
    "\n",
    "r2_train = sk.metrics.r2_score(train['votergap'], yhat1_train)\n",
    "r2_test = sk.metrics.r2_score(test['votergap'], yhat1_test)\n",
    "\n",
    "mse_train = sk.metrics.mean_squared_error(train['votergap'], yhat1_train)\n",
    "mse_test = sk.metrics.mean_squared_error(test['votergap'], yhat1_test)\n",
    "      \n",
    "results1 = pd.DataFrame(index = [\"train\",\"test\"])    \n",
    "results1['R-squared'] = [r2_train,r2_test]\n",
    "results1['MSE'] = [mse_train,mse_test]\n",
    "\n",
    "results1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q0.1** Calculate the residuals for the train set from the linear regression model.  Plot the histogram of residuals and residuals-vs.-predicted scatterplot.  Comment on the assumptions of the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual histogram and scatterplot below illustrate that the assumptions are reasonable (even though the residuals are moderately left-skewed, there are not any extreme outliers).  The residual scatterplot does not show any clear non-linearities (curvature) or non-constant variance (sometimes called heteroscedasticity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bootstrap in Linear Regression \n",
    "\n",
    "The residual histogram is clearly non-normal, and this may affect teh reliability of probabilistic based inference ($t$-distribution based ones).  The bootstrap approach is an alternative method to build confidence intervals when this assumption is violated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.1** Perform a bootstrap approach to calculate the 95\\% confidence intervals based on the method, and compare them to the probabilistic-based ones above from statsmodels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define some parameters and initializations:\n",
    "nboots = 1000\n",
    "n = np.size(train['votergap'])\n",
    "np.random.seed(109)\n",
    "beta1_boots = []\n",
    "\n",
    "# use a for loop to do the reampling for us\n",
    "for boot in np.arange(nboots):\n",
    "    indices = np.random.choice(n,size=n,replace=True)\n",
    "    ######\n",
    "    # your code here\n",
    "    ######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# plot the histogram and pull of the quantiles from the results array using np.quantile and \n",
    "\n",
    "######\n",
    "# your code here\n",
    "######\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, hypothesis testing can be performed with a similar resampling approach (call the permutation or randomization test).  As all hypothesis tests, the sampling distribution (of the statistic of interest: here it is $\\beta_1$) should be determined assuming the null hypothesis to be true (to fix the Type 1 error rate at $\\alpha = 0.05$).  \n",
    "\n",
    "Thus, rather than bootstrap resampling the obseervations from the data set, the association is removed empirically: by reshuffling the response variable across the measurements in the predictor variable(s).  One iteration is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.size(train['votergap'])\n",
    "y_perm = np.random.choice(train['votergap'],size=n,replace=False)\n",
    "regress_perm = LinearRegression(fit_intercept=True).fit(train[['log_density']], y_perm)\n",
    "\n",
    "print(\"Observed Beta1 =\", regress1.coef_, \", Permuted Beta1 =\", regress_perm.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that our permutaed $\\hat{\\beta}_1$ is much colser to the null value of zero (in magnitude) than the actual observed one.  Thus our empirically estimated p-value is 0 (our repermuted slope was more extreme than what was actually observed 0 times out of 1 permutation iterations).\n",
    "\n",
    "**Q1.2** Perform a permutation test (with 1000 iterations) to test whether the linear relationship between `votergap` and `log_density` is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multiple Regression\n",
    "\n",
    "There are many more predictors in the data set (see below), which are mostly percentage of residents in the county (income is median income in dollars, and cancer is number of cancer cases per 100,000 residents per year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.columns)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1** Fit a simple regression model to predict `votergap` from `log_minority`.  Interpret the slope coefficient and its confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.2** Fit a multiple regression model to predict `votergap` from `log_minority` and `log_density`.  Interpret the slope coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "###### \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.3** Compare the coefficient estimate for `log_density` in both the simple regression model and the multiple regression model.  Why are the different/similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Predictors, predictors, predictors\n",
    "\n",
    "In this part we will explore 3 very useful types of predictors commonly used in multiple regression (and modeling in general).\n",
    "\n",
    "1. Categorical Predictors (and dummy variables)\n",
    "2. Interaction Effects\n",
    "3. Polynomial Terms as one approach to model non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.1** Create two sets of dummy variables: \n",
    "1. A variable called `high_density` to indicate whether a county is above the median population density of all counties in the training set.\n",
    "\n",
    "2. For the 50 different states (+DC) in the `state` variable\n",
    "\n",
    "Note: pandas's [pd.get_dummies](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) will be useful for the second task.  Note: it would be useful to use the argument `drop_first=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.2** Fit a linear regression model to predict `votergap` from `high_density`.  Interpret the model coefficient estimates.  When would a model like this be preferred to using the quantitative version of `density` or `log_density`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.3** Fit a linear regression model to predict `votergap` from `state` (or the state dummies, whichever is easier) in both sklearn and in statsmodels.  Interpret the slope coefficient associated with Massachusetts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "\n",
    "# sklearn\n",
    "\n",
    "# statsmodels approach: remember to manually add the intercept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single interaction term is easy to calculate manually, but the job can be a chore if there are a lot of interaction terms you want to create.  Below is the code to create an interaction term both ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['minority_density_interact'] = train['log_minority']*train['log_density']\n",
    "\n",
    "X_interact = PolynomialFeatures(2, interaction_only=True, include_bias=False).fit_transform(train[['log_minority','log_density']])\n",
    "pd.DataFrame(X_interact).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.4** Fit a linear regression model to predict `votergap` from `log_minority`, `log_density` and their interaction.  Print out the coefficient estimates for this model and interpret the interaction term.  How does this model compare to the multiple regression model in Q2.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, polynomial terms are easy to calculate manually, but the job can be a chore if the order of polynomial function gets high.  Below is the code to create some polynomial terms both manually and using pd.PolynomialFeatures (note: PolynomialFeatures can be used to create interaction terms, polynomial terms, or both!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['log_minority_2'] = train['log_minority']**2\n",
    "train['log_minority_3'] = train['log_minority']**3\n",
    "\n",
    "X_poly = PolynomialFeatures(3, include_bias=False).fit_transform(train[['log_minority']])\n",
    "\n",
    "train[['log_minority','log_minority_2','log_minority_3']].head()\n",
    "#pd.DataFrame(X_poly).head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.5** Fit a 3rd order polynomial regression model to predict `votergap` from `log_minority`.  Print out the coefficient estimates and plot the prediction curve on top of the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n",
    "\n",
    "#do the predictions and plot them\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Variable Selection\n",
    "\n",
    "In this part we will step through forward variable selection.  Recall our original data set has several quantitative predictors (plus `state`):\n",
    "\n",
    "['population','hispanic', 'minority', 'female', \n",
    "             'unemployed', 'income', 'nodegree', 'bachelor', 'inactivity', 'obesity', 'density', \n",
    "             'cancer']\n",
    "\n",
    "Note: several of them are extremely right-skewed and could be better incorporated using the log: population, hispanic, minority, and density (these are the most etreme right-skewed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['population','hispanic', 'minority', 'female', \n",
    "             'unemployed', 'income', 'nodegree', 'bachelor', 'inactivity', 'obesity', 'density', \n",
    "             'cancer']\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(15, 15))\n",
    "\n",
    "count = 0\n",
    "for variable in predictors:\n",
    "    ax[count%4][int(count/4)].hist(train[variable],)\n",
    "    ax[count%4][int(count/4)].set_xlabel(variable)\n",
    "    count = count + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['log_population'] = np.log(train['population'])\n",
    "train['log_hispanic'] = np.log(train['hispanic'])\n",
    "\n",
    "test['log_population'] = np.log(test['population'])\n",
    "test['log_hispanic'] = np.log(test['hispanic'])\n",
    "\n",
    "# imputing median cancer rate for the 40 or so counties with missing cancer rates\n",
    "train['cancer'].loc[train['cancer'].isnull()] = np.median(train['cancer'])\n",
    "test['cancer'].loc[test['cancer'].isnull()] = np.median(train['cancer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.1** Fit the full *main effects* model with the 12 predictor variables listed below.  Determine its $R^2$ in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['log_population','log_hispanic', 'log_minority', 'female', \n",
    "             'unemployed', 'income', 'nodegree', 'bachelor', 'inactivity', 'obesity', 'log_density']\n",
    "\n",
    "\n",
    "######\n",
    "# your code here\n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.2** Fit the full *interaction* model with the 12 predictor variables and all of their interaction terms.  How many interaction terms are involved? Determine its $R^2$ in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there is no automatic way to do sequential variable selection in sklearn or statsmodels (we'll learn why eventually). One way to do manual variable selection is to fit the most complex model you want to consider, and start removing variables one at a time based on their p-value of their $t$-statistic, until all you are left with only statistically significant predictors (with p-values all less than 0.05) model.\n",
    "\n",
    "The code below shows a brief outline of how to get started with this process in statsmodels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First turn the full design matrix into a panda's DataFrame\n",
    "df4_3 = pd.DataFrame(X4_2)\n",
    "# print(df4_3.shape)\n",
    "\n",
    "# Fit the model with all predictors and determine which has the largest p-value\n",
    "model4_3_temp = lm.OLS(train['votergap'],df4_3).fit()\n",
    "print(np.argmax(model4_3_temp.pvalues))\n",
    "\n",
    "# you can confirm it with the summary output\n",
    "model4_3_temp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now start dropping variables, one at a time\n",
    "df4_3 = df4_3.drop(np.argmax(model4_3_temp.pvalues),axis=1)\n",
    "df4_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And refit, and redetermine the max p=value\n",
    "model4_3_temp = lm.OLS(train['votergap'],df4_3).fit()\n",
    "print(\"Predictor#: \", np.argmax(model4_3_temp.pvalues), \", with associated p-value of\" ,np.max(model4_3_temp.pvalues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.3** \n",
    "Wrap the above code in a function in order to find the .  Determine the $R^2$ in the test se for this parsimonious model.  How does it compare to the predictive ability of the models in Q4.2 and Q4.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "# your code here\n",
    "######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your answer here*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
